[[course-syllabus]]
Course Syllabus
---------------

[[model-order-reduction]]
Model Order Reduction
---------------------

[[definition]]
Definition
~~~~~~~~~~

Problem statement

Goal Replicate input-output behavior of large-scale system
latexmath:[\Sigma] over a certain (restricted) range of

* forcing inputs and
* parameter inputs

image:Slides/rbm/course-rom-definition.png[image]

Problem statement Given large-scale system
latexmath:[\Sigma_\mathcal{N}] of dimension latexmath:[\mathcal{N}],
find a reduced order model latexmath:[\Sigma_N] of dimension
latexmath:[N << \mathcal{N}] such that: The approximation error is
small, i.e., there exists a global error bound such that

* latexmath:[\|u(\mu) - u_N (\mu)\| \leq \varepsilon_{\mathrm{des}}],
and
latexmath:[|s(\mu) - s_N (\mu)| \leq \varepsilon^s_{\mathrm{des}} , \forall \mu \in D^{\mu}].
* Stability (and passivity) is preserved.
* The procedure is computationally stable and efficient.

[[motivation]]
Motivation
~~~~~~~~~~

Generalized Inverse Problem

* Given PDE(latexmath:[\mu]) constraints, find value(s) of parameter
latexmath:[\mu] which:
** (OPT) minimizes (or maximizes) some functional;
** (EST) agrees with measurements;
** (CON) makes the system behave in a desired manner;
** or some combination of the above
* Full solution computationally very expensive due to a repeated
evaluation for many different values of latexmath:[\mu]
* Goal: or

[[methodologies]]
Methodologies
~~~~~~~~~~~~~

Methodologies

* Reduced Basis Methods
* Proper Orthogonal Decomposition
* Balanced Truncation
* Krylov Subspace Methods
* Proper Generalized Decomposition
* Modal Decomposition
* Physical model reduction
* ...

Disclaimer Model Order Reduction Techniques

* replace your favorite discretization scheme (e.g. FE, FV, FD), but
instead are build upon and supplement these schemes.
* useful if you are interested in a single high-fidelity solution of
your high-dimensional problem, but instead if you are interested in the
many-query or real-time context.

[[some-examples]]
Some Examples
-------------

[[cooling-of-electronical-components]]
Cooling of electronical components
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Thermal Testcase Description

0.5

0.5

Overview

* Heat-Transfer with conduction and convection possibly coupled with
Navier-Stokes
* Simple but complex enough to contain all difficulties to test the
certified reduced basis
** non symmetric, non compliant
** steady/unsteady
** physical and geometrical parameters
** coupled models
* Testcase can be easily complexified

[[aerothermal-flows]]
Aerothermal flows
~~~~~~~~~~~~~~~~~

Airbus Use-Case

image:Slides/rbm/cabin_description[image] +

Some Scientific Issues

* Turbulence
* Mixed forced and natural convection
* Boundary conditions coupled to an ECS (Environment Control System)
* Error prediction (Reduced Basis)

[[modeling-of-high-field-magnets]]
Modeling of high field magnets
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 +

High Field Magnet Modeling

Laboratoire National des Champs MagnÃ©tiques Intenses

Large scale user facility in France

* High magnetic field : from 24 T
* Grenoble : continuous magnetic field (36 T)
* Toulouse : pulsed magnetic field (90 T)

3.4cm

Application domains

* Magnetoscience
* Solide state physic
* Chemistry
* Biochemistry

2.4cm

image:Figures/pngs/lncmi/Magnet_3D_Ouvert.png[image]

3.9cm

Magnetic Field

* Earth : latexmath:[5.8 \cdot 10^{-4} T]
* Supraconductors : latexmath:[24 T]
* 
* Pulsed field : latexmath:[90 T]

Access

* Call for Magnet Time : latexmath:[2 ~\times] per year
* latexmath:[\approx ~140] projects per year

3.5cm

4cm

image:Figures/pngs/lncmi/Model_3D.png[image]

5cm

image:Figures/pngs/lncmi/temp_picard_np1024_OT200l170_comp.png[image]

4.5cm

image:Figures/pngs/lncmi/Magnetmodels_bmap+dilat_HL31.png[image]

Why use Reduced Basis Methods ?

Challenges

* Modeling : multi-physics non-linear models, complex geometries,
genericity
* Account for uncertainties : uncertainty quantification, sensitivity
analysis
* Optimization : shape of magnets, robustness of design

4.8cm

Objective 1 : Fast

* Complex geometries
** Large number of dofs
* Uncertainty quantification
** Large number of runs

4.4cm

Objective 2 : Reliable

* Field quality
* Design optimization
** Certified bounds
** Reach material limits

[[summary]]
Summary
~~~~~~~

Summary Many problems in computational engineering require

many or real-time evaluations of PDE(latexmath:[\mu])-induced +
input-output relationships.

Model order reduction techniques enable

certified, real-time calculation +
of outputs of PDE(latexmath:[\mu]) +
for parameter estimation, optimization, and control.

[[reduced-basis-method]]
Reduced Basis Method
--------------------

[[problem-statement]]
Problem Statement
~~~~~~~~~~~~~~~~~

The Reduced Basis Method

* <2-> Comparison to other model reduction techniques:
** Parametrized problems(material, constants, geometry,...)
** A posteriori error estimation
** Offline-online decomposition
** Greedy algorithm (to construct reduced basis space)
* <3-> Motivation:
** Efficient solution of optimization and optimal control problems
governed by parametrized PDEs.

Problem Statement

The Main Idea - Key Observation

General Problem Statement Given a system
latexmath:[\Sigma_\mathcal{N}] of large dimension N,
image:Slides/rbm/gen-prob-fe.png[image] where

* latexmath:[u(\mu, t) \in \mathbb{R}^{\mathcal{N}}], the state
* latexmath:[s(\mu, t)], the outputs of interest
* latexmath:[g(t)], the forcing or control inputs

are functions of

* latexmath:[\mu \in D], the parameter inputs
* latexmath:[t], time

and the matrices latexmath:[M], latexmath:[A], latexmath:[B], and
latexmath:[L] also depend on latexmath:[\mu] latexmath:[\ldots]

General Problem Statement latexmath:[\ldots] construct a reduced order
system latexmath:[\Sigma_N] of dimension latexmath:[N
  << \mathcal{N}],

image:Slides/rbm/gen-prob-rb.png[image]

where latexmath:[u_N(\mu) \in \mathbb{R}^N] is the reduced state.

Special case We start by considering latexmath:[\dot{u} = 0]

*Full Model*

latexmath:[\begin{align}
          A(\mu) u(\mu)& = & F(\mu)\\
          s(\mu)&=&L^T(\mu) u(\mu)
        \end{align}]

*Reduced Model*

latexmath:[\begin{align}
          A_N(\mu) u_N(\mu)& = & F_N(\mu)\\
          s_N(\mu)&=&L^T_N(\mu) u_N(\mu)
        \end{align}]

[[key-ingredients]]
Key Ingredients
~~~~~~~~~~~~~~~

Approximation

* <1-> Take ``snapshots'' at different latexmath:[\mu]-values:
latexmath:[u(\mu_i), i = 1
  \ldots N], and let
latexmath:[Z_N=[\xi_1,\ldots,\xi_N] \in \mathbb{R}^{\mathcal{N}\times N}]
where the basis/test functions, latexmath:[\xi_i] ``latexmath:[=]''
latexmath:[u(\mu_i)], are orthonormalized
* <2-> For any new latexmath:[\mu], approximate latexmath:[u] by a
linear combination of the latexmath:[\xi_i]
latexmath:[u(\mu) \approx \sum_{i=1}^N u_{N,i}(\mu) \xi_i = Z_N u_N(\mu)]
determined by Galerkin projection, i.e.,

A posteriori error estimation

* <1-> Assume well-posedness; latexmath:[A(\mu)] pos.def. with min
eigenvalue latexmath:[\alpha_a :=\lambda_1 >0], where
latexmath:[A \xi=\lambda X \xi] and latexmath:[X] corresponds to the
latexmath:[X]-inner product, latexmath:[(v, v)_X = \|v\|_X^2]
* <2-> Let latexmath:[\underbrace{e_N = u - Z_N\ u_N}_{\text{error}}]
, and
latexmath:[\underbrace{r = F - A\ Z_N\ u_N}_{\text{residual}}, \forall \mu \in D],
so that latexmath:[A(\mu) e_N (\mu) = r(\mu)]
* <3-> Then for any latexmath:[\mu \in D],
latexmath:[\|u(\mu)- Z_N u_N(\mu)  \|_X \leq
      \frac{\|r(\mu)\|_{X'}}{\alpha_{LB}(\mu)} =: \Delta_N(\mu)]
latexmath:[|s(\mu)-s_N(\mu)| \leq \|L\|_{X'} \Delta_N(\mu) =: \Delta^s_N(\mu)]
where latexmath:[\alpha_{LB}(\mu)] is a lower bound to
latexmath:[\alpha_a(\mu)], and latexmath:[\|r\|_{X'}=r^T X^{-1} r].

Offline-Online decomposition

Greedy Algorithm

[[summary-1]]
Summary
~~~~~~~

Reduced Basis Opportunities Computational Opportunities

* We restrict our attention to the typically smooth and low-dimensional
manifold induced by the parametric dependence. +
latexmath:[\Rightarrow] Dimension reduction
* We accept greatly increased offline cost in exchange for greatly
decreased online cost. +
latexmath:[\Rightarrow] Real-time and/or many-query context

Reduced Basis Relevance Real-Time Context
(control,latexmath:[\ldots]): latexmath:[\begin{align}
    \mu & \rightarrow & s_N(\mu), \Delta^s_N(\mu)  & \\
    t_0 (``input'') & & & t_0+\delta t_{\mathrm{comp}} (``response'')
    \end{align}] Many-Query Context (design,latexmath:[\ldots]):
latexmath:[\begin{align}
    \mu_j  & \rightarrow & s_N(\mu_j), \Delta^s_N(\mu_j),\quad
    j=1\ldots J  \\  
    t_0  & & t_0+\delta t_{\mathrm{comp}} J\quad (J \rightarrow \infty)
    \end{align}] latexmath:[\Rightarrow] (real-time) and/or
(many-query) .

Reduced Basis Challenges

* A Posteriori error estimation
** Rigorous error bounds for outputs of interest
** Lower bounds to the stability ``constants''
* Offline-online computational procedures
** Full decoupling of finite element and reduced basis spaces
** A posteriori error estimation
** Nonaffine and nonlinear problems
* Effective sampling strategies
** High parameter dimensions

Reduced Basis Outline

1.  Affine Elliptic Problems
* (non)symmetric, (non)compliant, (non)coercive
* (Convection)-diffusion, linear elasticity, Helmholtz
2.  Affine Parabolic Problems
* (Convection)-diffusion equation
3.  Nonaffine and Nonlinear Problems
* Nonaffine parameter dependence, nonpolynomial nonlinearities
4.  Reduced Basis (RB) Method for Fluid Flow
* Saddle-Point Problems (Stokes)
* Navier-Stokes Equations
5.  Applications
* Parameter Optimization and Estimation (Inverse Problems)
* Optimal Control

[[section]]

Linear Compliant Elliptic Problems

[[notations-definitions-problem-statement-example]]
Notations, Definitions, Problem Statement, Example
--------------------------------------------------

[[inner-product-spaces]]
Inner Product Spaces
~~~~~~~~~~~~~~~~~~~~

Definitions

A space latexmath:[Z] is a linear or vector space if, for any
latexmath:[\alpha \in
    \mathbb{R}] , latexmath:[w,v \in Z],
latexmath:[\alpha w+v \in Z]

Note: latexmath:[\mathbb{R}] denotes the real numbers, and
latexmath:[\mathbb{N}] and latexmath:[\mathbb{C}] shall denote the
natural and complex numbers, respectively.

An inner product space (or Hilbert space) latexmath:[Z] is a linear
space equipped with

* an inner product latexmath:[(w,v)_Z, \forall w,v \in Z],and
* induced norm latexmath:[\|w\|_Z = (w,w)_Z, \forall w \in Z].

Inner Product

An inner product
latexmath:[w,v \in Z \rightarrow (w,v)_Z \in \mathbb{R}] has to
satisfy

* Bilinearity +
+
latexmath:[(\alpha w+v,z)_Z =\alpha(w,z)_Z +(v,z)_Z \forall \alpha\in R,w,v,z\in Z]
+
 +
+
latexmath:[(z,\alpha w+v)_Z =\alpha(z,w)_Z +(z,v)_Z, \forall \alpha\in R, w,v,z\in Z ]
* Symmetry +
+
latexmath:[(w,v)_Z = (v,w)_Z, \forall w,v \in Z]
* Positivity +
+
latexmath:[(w,w)_Z >0, \forall w \in Z, w \neq 0]
+
 +
+
latexmath:[(w,w)_Z =0, \text{ only if } w=0]

Cauchy-Schwarz inequality:
latexmath:[(w,v)_Z \leq \|w\|_Z\|v\|_Z,\forall w, v \in Z.]

Norm

A norm is a map latexmath:[\| \cdot \| : Z \rightarrow \mathbb{R}]
such that

* latexmath:[\|w\|_Z > 0\quad \forall w\in Z,w\neq 0,]
* latexmath:[\|\alpha w\|_Z = |\alpha |\|w\|_Z\quad \forall \alpha \in
      \mathbb{R},\ \forall w\in Z, ]
* latexmath:[\|w+v\|_Z \leq \|w\|_Z +\|v\|_Z\quad \forall w\in Z,\ \forall v\in Z.]

Equivalence of norms latexmath:[\| \cdot \|_Z] and
latexmath:[\| \cdot \|_Y] : there exist positive constants
latexmath:[C_1], latexmath:[C_2] such that
latexmath:[C_1\|v\|_Z \leq \|v\|_Y \leq C_2\|v\|_Z .]

Cartesian Product Space Given two inner product spaces latexmath:[Z_1]
and latexmath:[Z_2], we define
latexmath:[Z = Z_1 \times Z_2  \equiv  \{(w_1,w_2)\ | \ w_1 \in  Z_1,\ w_2 \in  Z_2\}]
and given latexmath:[w = (w_1,w_2) \in  Z, v = (v_1,v_2) \in  Z], we
define latexmath:[w + v  \equiv  (w_1 + v_1, w_2 + v_2).] We also
equip latexmath:[Z] with the inner product
latexmath:[(w,v)_Z =(w_1,v_1)_{Z_1} +(w_2,v_2)_{Z_2}] and induced
norm latexmath:[\|w\|_Z = (w,w)_Z.]

[[linear-and-bilinear-forms]]
Linear and Bilinear Forms
~~~~~~~~~~~~~~~~~~~~~~~~~

Linear Forms

A functional latexmath:[g : Z \rightarrow  \mathbb{R}] is a linear
functional if, for any latexmath:[\alpha   \in  \mathbb{R}, w,
  v  \in  Z] latexmath:[g(\alpha w + v) = \alpha g(w) + g(v)]

A linear form is bounded, or continuous, over latexmath:[Z] if
latexmath:[|g(v)| \leq  C \|v\|_Z, \forall v \in  Z,] for some
finite real constant latexmath:[C].

Dual Spaces

Given latexmath:[Z], we define the dual space latexmath:[Z'] as the
space of all bounded linear functionals over latexmath:[Z]. We
associate to latexmath:[Z'] the dual norm
latexmath:[\|g\|_{Z'} = \sup_{v \in Z} \frac{g(v)}{\|v\|_Z} , \forall g \in
    Z'.]

For any latexmath:[g \in Z'], there exists a unique
latexmath:[w_g \in Z] such that
latexmath:[(w_g, v)_Z =g(v), \forall v \in Z.]

It directly follows that latexmath:[\|g\|_{Z'} = \|w_g\|_Z.]

Bilinear Forms

A form latexmath:[b:Z_1 \times Z_2 \rightarrow \mathbb{R} ] is
bilinear if, for any latexmath:[\alpha \in R],

* latexmath:[b(\alpha w + v,z) = \alpha b(w,z) + b(v,z), \forall w,v \in  Z_1,
    z \in  Z_2 ]
* latexmath:[b(z,\alpha w + v) =\alpha b(z,w) + b(z,v), \forall z \in  Z_1, w,v \in  Z_2]

The bilinear form latexmath:[b : Z \times Z \rightarrow \mathbb{R}] is

* symmetric, if latexmath:[b(w,v) = b(v,w),]
* skew-symmetric, if latexmath:[b(w,v) = -b(v,w),]
* positive definite, if
latexmath:[b(v,v) \geq  0\text{ , with equality only for } v = 0.]

Bilinear Forms The bilinear form
latexmath:[b : Z \times Z \rightarrow \mathbb{R}] is positive
semidefinite, if latexmath:[b(v,v) \geq  0, \forall v  \in  Z.] We
also define, for a general bilinear form latexmath:[b : Z \times Z
  \rightarrow \mathbb{R}], the

* symmetric part as
latexmath:[b_S(w,v) = 1/2 (b(w,v) + b(v,w)), \forall w,v \in Z;]
* the skew-symmetric part as
latexmath:[b_{SS}(w,v) = 1/2 (b(w,v) - b(v,w)), \forall w,v \in Z.]

Bilinear Forms The bilinear form
latexmath:[b : Z \times Z \rightarrow \mathbb{R}] is

* over latexmath:[Z] if
latexmath:[\alpha \equiv \inf_{w\in Z} \frac{b(w,w)}{\|w\|^2_Z}] is
positive;
* over latexmath:[Z] if
latexmath:[\gamma \equiv \sup_{w\in Z}  \sup_{v\in Z} \frac{b(w, v)}{\|w\|_Z \|v\|_Z}]
is finite.

Parametric Linear and Bilinear Forms We introduce

* latexmath:[D  \in  \mathbb{R}^P] : closed bounded parameter domain;
* latexmath:[\mu = (\mu_1,\ldots,\mu_P)  \in  D] : parameter vector.

We shall say that

* latexmath:[g:Z\times D\rightarrow \mathbb{R}] is a if, for all
latexmath:[\mu  \in  D, g( \cdot ; \mu) : Z \rightarrow  \mathbb{R}]
is a linear form;
* latexmath:[b:Z\times Z\times D\rightarrow \mathbb{R}] is a if,for
all
latexmath:[\mu  \in  D, b( \cdot ,  \cdot ; \mu) : Z \times  Z \rightarrow  \mathbb{R}]
is a bilinear form.

Concepts of symmetry,latexmath:[\ldots] directly extend to the
parametric case.

Parametric Linear and Bilinear Forms The parametric bilinear form
latexmath:[b : Z \times Z \times D \rightarrow
  \mathbb{R}] is

* coercive over Z if
latexmath:[\alpha(\mu) \equiv  \inf_{w \in Z} \frac{b(w,w;\mu)}{\|w\|^2_Z}]
is positive for all latexmath:[\mu  \in  D];
* continuous over latexmath:[Z] if
latexmath:[\gamma(\mu)\equiv  \sup_{w \in Z} \sup_{v \in Z} \frac{b(w, v; \mu)}{\|w\|_Z\|v\|_Z}]
is finite for all latexmath:[\mu  \in  D.]

We also define latexmath:[\begin{align}
(0 <) \alpha _0 & \equiv  \min_{\mu \in D} \alpha (\mu)\\
\gamma_0 & \equiv \max_{\mu \in D} \gamma (\mu) (< \infty ).
    \end{align}]

Coercivity EigenProblem We have
latexmath:[\alpha (\mu) \equiv  \inf_{w \in Z} \frac{b_S(w,w;\mu)}{\|w\|^2_Z}]

Associated generalized eigenproblem:

Given latexmath:[\mu  \in  D], find
latexmath:[(\chi^{co},\nu^{co})_i(\mu)  \in  Z \times
\mathbb{R}, 1 \leq  i \leq  \dim(Z),] such that
latexmath:[b_S(\chi_i^{co}(\mu), v; \mu) = \nu_i^{co}(\mu)(\chi_i^{co}(\mu), v)_Z]
and latexmath:[\|\chi_i^{co}(\mu)\|_Z=1] Let
latexmath:[\nu_1^{co}(\mu) \leq  \nu_2^{co}(\mu) \leq  \ldots \leq  \nu_{\dim{Z}}^{co} (\mu)]
and b coercive, then latexmath:[\alpha (\mu) = \nu_1^{co}(\mu) > 0.]

Parameter affine Dependence We assume
latexmath:[g(v;\mu)= \sum_{q=1}^{Q_g} \theta^q_g(\mu)g^q(v), \forall v \in Z,]
where, for latexmath:[1 \leq  q \leq  Q_g] (finite),

* functions latexmath:[\theta^q_g : D
     \rightarrow  \mathbb{R}],
* forms latexmath:[g^q : Z \rightarrow  \mathbb{R};]

and
latexmath:[b(w,v;\mu)= \sum_{q=1}^{Q_b} \theta^q_b(\mu) b^q(w,v),\quad \forall
w,v \in Z,] where, for latexmath:[1 \leq  q \leq  Q_b] (finite),

* functions latexmath:[\theta^q_b : D \rightarrow  \mathbb{R}],
* forms latexmath:[b^q : Z \times  Z \rightarrow  \mathbb{R}].

Parametric Coercivity

The coercive bilinear form
latexmath:[b : Z \times Z \times D \rightarrow
\mathbb{R}]
latexmath:[b(w,v;\mu)= \sum_{q=1}^{Q_b} \theta^q_b(\mu) b^q(w,v),\quad \forall
w,v \in Z,] is if latexmath:[c\equiv  b_S] is affine
latexmath:[c(w,v;\mu)= \sum_{q=1}^{Q_c} \theta^q_c(\mu) c^q(w,v),\quad \forall
w,v \in Z,] and satisfies and

* latexmath:[\theta^q_c(\mu)>0, \forall \mu \in D, 1\leq q\leq Q_c,]
* latexmath:[c^q(v,v)\geq 0,\forall v \in Z, 1\leq q\leq Q_c.]

[[classes-of-functions]]
Classes of Functions
~~~~~~~~~~~~~~~~~~~~

Scalar and Vector Fields We consider (real)

* scalar-valued field variables (e.g., temperature, pressure)
latexmath:[w : \Omega  \rightarrow  \mathbb{R}^{d=1}]
* vector-valued field variables (e.g., displacement, velocity)
latexmath:[\mathbf{w} : \Omega  \rightarrow \mathbb{R}^d] , where
latexmath:[\mathbf{w}(x) = (w_1(x), \ldots , w_d (x));]

and

* latexmath:[\Omega  \in \mathbb{R}^d, d=1, 2, \text{or} 3] is an open
bounded domain
* latexmath:[x = (x_1,...,x_d)  \in  \Omega ];
* latexmath:[\Omega] has Lipschitz continuous boundary
latexmath:[\partial \Omega] ; and
* we define the canonical basis vectors as
latexmath:[e_i, 1 \leq  i \leq  d.]

Multi-Index Derivative Given a scalar (or one component of a vector)

* field latexmath:[w : \Omega  \rightarrow  \mathbb{R}]SPATIAL
DERIVATIVE
latexmath:[(D^\sigma w)(x) = \frac{\partial^\sigma w}{\partial x_1^{\sigma_1} ...\partial x_d^{\sigma d}}]
* field latexmath:[w : \Omega  \times  D \rightarrow  \mathbb{R}]
SENSITIVITY DERIVATIVE
latexmath:[(D_\sigma w)(x) = \frac{\partial^\sigma w}{\partial \mu_1^{\sigma_1} ...\partial \mu_d^{\sigma d}}]

where

* latexmath:[\sigma  = (\sigma_1,\ldots,\sigma_d)],
latexmath:[\sigma_i, 1 \leq  i \leq  d], non-negative integers;
* latexmath:[|\sigma| = \sum_{j=1}^{d} \sigma_j] is the order of the
derivative; and
* latexmath:[I^{d,n}] is set of all index vectors
latexmath:[\sigma   \in  N^d_0] such that
latexmath:[|\sigma | \leq  n.]

Function Spaces

Let latexmath:[m  \in  N_0], the space latexmath:[C^m(\Omega )] is
defined as
latexmath:[C^m(\Omega )\equiv  \{w | D^\sigma w  \in  C^0(\Omega ), \forall \sigma   \in  I^{d,m}\},]
and latexmath:[C^0(\Omega )] is the space of continuous functions over
latexmath:[\Omega   \in  \mathbb{R}^d].

We denote by latexmath:[C^\infty (\Omega )] the space of functions
latexmath:[w] for which latexmath:[D^\sigma] exists and is
continuous for any order latexmath:[|\sigma |.]

Lebesgue Spaces

We define, for latexmath:[1 \leq  p < \infty] , the Lebesgue space
latexmath:[L^p(\Omega
)] as
latexmath:[L^p(\Omega )\equiv \{  w \text{ measurable } |\quad  \|w\|_{L^p(\Omega )} < \infty\}]
where

* latexmath:[\|w\|_{L^p(\Omega )} \equiv  \left( \int_\Omega |w|^pdx\right)^{1/p} ,
    1\leq p<\infty,]
* latexmath:[\|w\|_{L^\infty (\Omega )} \equiv \mathrm{ess} \sup_{x\in\Omega} |w(x)|, p = \infty .]

Hilbert Space

Let latexmath:[m  \in  \mathbb{N}_0], the space
latexmath:[H^m(\Omega )] is then defined as
latexmath:[H^m(\Omega )\equiv  \{w |\quad D^\sigma w  \in   L^2(\Omega ), \forall \sigma   \in  I^{d,m}\},]
with associated inner product
latexmath:[(w,v)_{H^m(\Omega )}\equiv \sum_{\sigma  \in I^{d,m}}\int_\Omega  D^\sigma w D^\sigma v dx,]
and induced norm
latexmath:[\|w\|_{H^m(\Omega )} \equiv   \sqrt{(w, w)_{H^m(\Omega )}}.]

Special (most important) cases Since we only consider , we require
mostly

* latexmath:[L^2(\Omega )  = H^0(\Omega )]: Lebesgue Space
latexmath:[p = 2]
latexmath:[(w,v)_{L^2(\Omega)} = \int_\Omega w v \quad \forall w, v  \in  L^2(\Omega )]
latexmath:[\|w\|_{L^2(\Omega)} = \sqrt{(w,w)_{L^2(\Omega)}} \forall w  \in  L^2(\Omega ),]
latexmath:[\Rightarrow] Space of all functions
latexmath:[w : \Omega  \rightarrow
\mathbb{R}] square-integrable over latexmath:[\Omega] .

Special (most important) cases Since we only consider , we require
mostly

* latexmath:[H^1(\Omega)]
latexmath:[H^1(\Omega ) \equiv \{w \in L^2(\Omega )| \frac{\partial w}{ \partial xi}  \in L^2(\Omega ), 1\leq i\leq d\}]
with inner product and induced norm
latexmath:[(w,v)_{H^1(\Omega )}  \equiv \int_\Omega \nabla w  \cdot   \nabla v
+ wv\quad \forall w,v  \in  H^1(\Omega ),],
latexmath:[\|w\|_{H^1(\Omega )}  \equiv \sqrt{(w,w)_{H^1(\Omega)}}\quad \forall w  \in  H^1(\Omega ),]
and seminorm
latexmath:[|w|_{H^1(\Omega )} \equiv \int_\Omega  \nabla w  \cdot   \nabla
w,\quad \forall w  \in  H^1(\Omega ).]

Special (most important) cases Since we only consider , we require
mostly

* the space latexmath:[H_0^1(\Omega )]
+
latexmath:[H^1_0(\Omega) \equiv \{v \in H^1(\Omega )|v_{|\partial \Omega}=0 \}]
where latexmath:[v = 0] on the boundary
latexmath:[\partial \Omega .]
+
Note that, for any latexmath:[v  \in  H_0^1(\Omega )], we have
latexmath:[C_{PF} \|v\|_{H^1(\Omega )} \leq  |v|_{H^1(\Omega )} \leq  \|v\|_{H^1(\Omega )},]
and thus latexmath:[\|v\|_{H^1(\Omega)} = 0 \, \Rightarrow v = 0]
latexmath:[\Rightarrow |v|_{H^1(\Omega )}] constitutes a norm for
latexmath:[v  \in
H_0^1(\Omega ).]

Projection

Given Hilbert Spaces latexmath:[Y] and latexmath:[Z \subset  Y] ,
the projection, latexmath:[\Pi  : Y \rightarrow  Z], of
latexmath:[y \in Y] onto latexmath:[Z] is defined as
latexmath:[(\Pi y,v)_Y = (y,v)_Y , \forall v  \in  Z]

Properties:

* Orthogonality: latexmath:[(y - \Pi y, v)_Y = 0]
* Idempotence: latexmath:[\Pi (\Pi y) = \Pi y]
* Best Approximation
latexmath:[\|y-\Pi y\|^2_Y = \inf_{v \in Z} \|y-v\|^2_Y, \, ]

Given an orthonormal basis
latexmath:[\{ \varphi i\}_{i=1, N = \dim(Z)}], then
latexmath:[\Pi y= \sum_{i=1}^{\dim(Z)}  ( \varphi i,y)_Y  \varphi_i, \forall y \in Y]

[[notations]]
Notations
~~~~~~~~~

Notations and Definitions

Notations

* latexmath:[(\cdot)^\mathcal{N}] finite element approximation
* latexmath:[(\cdot)_N] reduced basis approximation
* latexmath:[\mu] input parameter (physical, geometrical,...)
* latexmath:[s(t;\mu) \approx s^\mathcal{N}(t;\mu)\approx s_N(t;\mu) ]
output approximations
* latexmath:[\mu \rightarrow s(t;\mu)] input-output relationship

Definitions

* latexmath:[\Omega \subset \mathbb{R}^d] spatial domain
* latexmath:[\mu] latexmath:[P]-uplet
* latexmath:[\mathcal{D}^\mu \subset \mathbb{R}^P ] parameter space
* latexmath:[s] output, latexmath:[\ell, f] functionals
* latexmath:[u] field variable
* latexmath:[X] function space
latexmath:[H^1_0(\Omega)^\nu \subset X \subset H^1(\Omega)^\nu]
(latexmath:[\nu=1] for simplicity) +
latexmath:[(\cdot,\cdot)_X] scalar product and
latexmath:[\|\cdot\|_X] norm associated to latexmath:[X]

[[problem-statement-1]]
Problem Statement
~~~~~~~~~~~~~~~~~

Problem Statement

The formal problem statement reads: Given
latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}], evaluate
latexmath:[s(\mu)  = \ell(u(\mu);\mu)] where
latexmath:[u(x;\mu) \in X] satisfies
latexmath:[a(u(\mu), v; \mu ) = f(v; \mu), \quad \forall v \in X]

[rem:problem-statement] We consider first the case of linear affine
compliant elliptic problem and then complexify

Hypothesis: Reference Geometry In these notes latexmath:[\Omega] is
considered

* To apply the reduced basis methodology exposed later, we need to setup
a reference spatial domain latexmath:[\Omega_{\mathrm{ref}}]
* We introduce an affine mapping
latexmath:[\matcal{T}(\cdot;\mu) : \Omega (\equiv
    \Omega_{\mathrm{ref}} = \Omega_o(\bar{\mu}))
    \rightarrow \Omega_o(\mu)] such that
latexmath:[a(u,v;\mu) = a_o(u_o \circ \mathcal{T}_\mu,v_o \circ \mathcal{T}_\mu;\mu)]

Hypothesis: Continuity, stability, compliance We consider the following
latexmath:[\mu-]PDE

rl latexmath:[a(\cdot,\cdot;\mu)] & bilinear +
& symmetric +
& continuous +
& coercive
(latexmath:[\forall \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]) +
 +
latexmath:[f(\cdot;\mu), \ell(\cdot;\mu)] & linear +
& bounded
(latexmath:[\forall \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]) +

and in particular, to start, the compliant case

* latexmath:[a] symmetric
* latexmath:[f(\cdot;\mu) = \ell(\cdot;\mu)\quad \forall \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]

Hypothesis: Affine dependence in the parameter We require for the RB
methodology
latexmath:[a(u,v;\mu) = \sum_{q=1}^{Q_a} \Theta^q_a(\mu)\ a^q( u, v ),]
where for latexmath:[q=1,...,Q_a] latexmath:[\begin{array}[c]{rll}
      \Theta^q_a :& {\ensuremath{\mathcal{D}^\mu}\xspace}\rightarrow \mathbb{R} & \mu-\text{\alert{dependent} functions}\\
      a^q :& X \times X \rightarrow \mathbb{R} & \mu-\text{\alert{independent} bilinear forms}
    \end{array}]

[rem:hypothesis-affine]

* similar decomposition is required for latexmath:[\ell(v;\mu)] and
latexmath:[f(v;\mu)], and denote latexmath:[Q_\ell] and
latexmath:[Q_f] the corresponding number of terms
* applicable to a large class of problems including geometric variations
* can be relaxed (see non affine/non linear problems)

Inner Products and Norms

* and associated norm () latexmath:[\begin{aligned}
      (((w,v)))_\mu &=  a(w,v;\mu) &\ \forall u,v \in X\\
      |||v|||_\mu &=  \sqrt{a(v,v;\mu)} &\ \forall v \in X
    \end{aligned}]
* latexmath:[X]-inner product and associated norm ()
latexmath:[\begin{aligned}
      (w,v)_X &=  (((w,v)))_{\bar{\mu}} \ (\equiv a(w,v;\bar{\mu})) &\ \forall u,v \in X\\
      ||v||_X &=  |||v|||_{\bar{\mu}} \ (\equiv \sqrt{a(v,v;\bar{\mu})}) & \ \forall v \in X
    \end{aligned}]

Coercivity and Continuity Constants

We assume latexmath:[a] and

Recall that

* constant
latexmath:[(0 < ) \alpha(\mu) \equiv \inf_{v\in X}\frac{a(v,v;\mu)}{||v||^2_X}]
* constant
latexmath:[\gamma(\mu) \equiv \sup_{w\in X} \sup_{v\in X}\frac{a(w,v;\mu)}{\|w\|_X
      \|v\|_X} ( < \infty)]

[[example]]
Example
~~~~~~~

[[example-thermal-block-heat-transfer]]
Example Thermal Block: Heat Transfer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.6

(0,0) rectangle (3,3); (0,0) grid (3,3); (0,0) rectangle (3,3);

(1.5,-0.5)node[right]latexmath:[\Gamma_0] (Heat Flux)
to[out=180,in=90] (1.5,0);
(1.5,3.5)node[right]latexmath:[{\Gamma_{\mathrm{top}}}] (Zero
Dirichlet) to[out=180,in=90] (1.5,3); (3.5,1.5)
node[right]latexmath:[{\Gamma_{\mathrm{sides}}}] (Insulated)
to[out=180,in=90] (0,1.5) ; (3.5,1.5) to[out=180,in=-90] (3,1.5);

in 5mm,15mm,25mm

in 5mm,15mm,25mm

at (+0.45,+0.3) latexmath:[\mu_\theindex];

.3

Example Thermal Block: Problem statement Given
latexmath:[\mu \in (\mu_1,...\mu_P) \in {\ensuremath{\mathcal{D}^\mu}\xspace}\equiv
  [\mu^{\text{min}},\mu^{\text{max}}]^P], evaluate (recall that
latexmath:[\ell = f]) +
latexmath:[s(\mu) = f(u(\mu))] where
latexmath:[u(\mu) \in X \equiv \{ v \in H^1(\Omega), v|_{\Gamma_{\text{top}}}  =  0\}]
satisfies
latexmath:[a(u(\mu), v; \mu) = f(v;\mu) \quad \forall v \in X] we
have latexmath:[P = 8] and given latexmath:[1 < \mu_r < \infty] we
set
latexmath:[\mu^{\mathrm{min}} = 1/\sqrt{\mu_r},\quad \mu^{\mathrm{max}} =
    \sqrt{\mu_r}] such that
latexmath:[\mu^{\mathrm{max}}/\mu^{\mathrm{min}}=\mu_r.]

Example Thermal Block Recall we are in the compliant case
latexmath:[\ell = f], we have
latexmath:[f(v) = \int_{\Gamma_{0}} v\quad \forall v \in X] and
latexmath:[a(u,v;\mu) = \sum_{i=1}^{P} \mu_i \int_{\Omega_i} \nabla u \cdot \nabla v + 1 \int_{\Omega_{P+1}} \nabla u \cdot \nabla v
    \quad\forall u,\ v\ \in X] where
latexmath:[\Omega = \cup_{i=1}^{P+1} \Omega_i].

Example Thermal Block The inner product is defined as follows
latexmath:[(u,v)_X = \sum_{i=1}^P \bar{\mu}_i \int_{\Omega_i}\nabla u \cdot \nabla v + 1 \int_{\Omega_{P+1}} \nabla u \cdot \nabla v]
where latexmath:[\bar{\mu}_i] is a . We have readily that
latexmath:[a] is

* 
* latexmath:[0 < \frac{1}{\sqrt{\mu_r}} \leq \mathrm{min}(\mu_1/\bar{\mu}_1, \ldots,
      \mu_P/\bar{\mu}_P,1) \leq \alpha(\mu)]
* and
latexmath:[\gamma(\mu) \leq \mathrm{max}(\mu_1/\bar{\mu}_1, \ldots,
      \mu_P/\bar{\mu}_P,1) \leq \sqrt{\mu_r} < \infty]

and the linear form latexmath:[f] is .

Example Thermal Block: Affine decomposition We
latexmath:[a(u,v;\mu) = \sum_{q=1}^{P+1} \Theta^q(\mu) a^q(u,v)]
with latexmath:[\begin{aligned}
    \Theta^1(\mu) = \mu_1 & & a^1(u,v) = \int_{\Omega_1} \nabla u \cdot \nabla v\\
    & \vdots & \\
    \Theta^P(\mu) = \mu_P & & a^P(u,v) = \int_{\Omega_P} \nabla u \cdot \nabla v\\
    \Theta^{P+1}(\mu) = 1 & & a^{P+1}(u,v) = \int_{\Omega_{P+1}} \nabla u \cdot \nabla v
  \end{aligned}]

Example Thermal Block

* 0.5
** 
+
image:Figures/pngs/veys/thermalblock/33-max.png[image]
+
0.5
** 
+
image:Figures/pngs/veys/thermalblock/33-min.png[image]
* image:Figures/pngs/veys/thermalblock/33-random.png[image]

[sec:fem-approximation]

``Truth'' FEM Approximation

Let latexmath:[\mu \in \mathcal{D}^{\mu}], evaluate
latexmath:[\displaystyle s^{\mathcal{N}} (\mu) = \ell (u^{\mathcal{N}} (\mu)) \ ,]
where latexmath:[u^{\mathcal{N}} (\mu) \in X^{\mathcal{N}}] satisfies
latexmath:[a (u^{\mathcal{N}} (\mu), v; \mu ) = f (v), \quad \forall \: v \in X^{\mathcal{N}} \ .]
Here latexmath:[X^{\mathcal{N}} \subset X] is a finite element
approximation of dimension equiped with an inner product
latexmath:[(\cdot,\cdot)_X] and induced norm
latexmath:[||\cdot||_X]. Denote also latexmath:[X'] and associated
norm
latexmath:[\ell \in X',\qquad\displaystyle ||\ell||_{X'} \equiv \operatorname{sup}_{v\in X}\frac{\ell(v)}{||v||_X}].

Purpose

* latexmath:[u(\mu)] and latexmath:[u_{\mathcal{N}}(\mu)] in the
sense that
latexmath:[||u(\mu)-u_{\mathcal{N}}(\mu)||_X \leq \mathrm{tol}\quad\forall \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]
* the reduced basis approximation using the FEM approximation
* the error associated with the reduced basis approximation relative to
the FEM approximation

latexmath:[\Rightarrow u^{\mathcal{N}} (\mu)] is a calculable
surrogate for latexmath:[u(\mu).]
latexmath:[\|u(\mu)-u^\mathcal{N}(\mu)\|_{X} \leq
\underbrace{\|u(\mu)-u^\mathcal{N}(\mu)\|_{X}}_{\leq \varepsilon^\mathcal{N}} + \underbrace{\|u^\mathcal{N}(\mu)-u^N(\mu)\|_X}_{\varepsilon_{\mathrm{tol,min}}}]

with
latexmath:[\varepsilon^\mathcal{N} << \varepsilon_{\mathrm{tol,min}}]

[[sec:reduc-basis-appr]]
Reduced Basis Approximation
---------------------------

Reduced Basis Objectives For given accuracy latexmath:[\epsilon],
evaluate
latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}\rightarrow s_N(\mu) (\approx s^\mathcal{N}(\mu)) \text{ and }
    \Delta^s_N(\mu)] that achieves the desired accuracy Reliability
latexmath:[|s^\mathcal{N}(\mu)-s_N(\mu)| \leq \Delta^s_N(\mu) \leq \epsilon]
for a latexmath:[t_{\textsc{comp}}] Efficiency +
latexmath:[\text{\alert{Independent} of } \mathcal{N} \text{ as } \mathcal{N}
    \rightarrow \infty] where latexmath:[t_{\textsc{comp}}] is the
time to perform the input-output relationship
latexmath:[\hfill\mu \rightarrow (s_N(\mu),\Delta^s_N(\mu))]

Reduced Basis Objective : Rapid Convergence Build a rapidly convergent
approximation of
latexmath:[s_N(\mu) \in \mathbb{R} \text{ and } u_N(\mu) \in X^N \subset  X^{\mathcal{N}} \subset X]
such that for all latexmath:[\mu], we have
latexmath:[s_N(\mu) \rightarrow s^{\mathcal{N}}(\mu) \text{ and } u_N(\mu) \rightarrow u^{\mathcal{N}}(\mu)]
rapidly as
latexmath:[N = {\ensuremath{{\operatorname{dim}}}\xspace}{X_N} \rightarrow \infty (= 10-200)]
(and of latexmath:[\mathcal{N}])

Reduced Basis Objective : Reliability and Sharpness Provide error bound
latexmath:[\Delta_N(\mu)] and latexmath:[\Delta^s_N(\mu)] :
latexmath:[1 (\text{rigor}) \leq \frac{\Delta_N(\mu)}{\|u^{\mathcal{N}}(\mu)
      - u_N(\mu)\|_X} \leq \ E (\text{sharpness})] and
latexmath:[1 (\text{rigor}) \leq \frac{\Delta^s_N(\mu)}{|s^{\mathcal{N}}(\mu)
      - s_N(\mu)|} \leq \ E (\text{sharpness})] for all
latexmath:[N = 1 \ldots N_{\textsc{max}}] and
latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}].

Reduced Basis Objective : Efficiency Develop a two stage strategy :
Offline/Online

Offline:::
  very expensive pre-processing, we have typically that for a given
  latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]
  latexmath:[t^{\textsc{offline}}_{\textsc{comp}} >> t^{\mu\rightarrow s^{\mathcal{N}}(\mu)}_{\textsc{comp}}]
Online:::
  very rapid convergent certified reduced basis input-output
  relationship
  latexmath:[t^{\textsc{online}}_{\textsc{comp}} \text{ independent of } \mathcal{N}]

[rem:rbobjectives-efficiency] latexmath:[\mathcal{N}] may/should be
chosen

Parametric Manifold latexmath:[\mathcal{M}^\mathcal{N}] We assume

* the form latexmath:[a] is continuous and coercive (or inf-sup
stable); and
* affine -dependence; and
* the latexmath:[\theta^q(\mu), 1 \leq q \leq Q], are smooth (i.e.,
latexmath:[\theta^q \in C^\infty(\mathcal{D})] ;

then
latexmath:[\mathcal{M}^\mathcal{N} = \{ u^\mathcal{N}(\mu),\, \mu \in \mathcal{D}\}]
is a smooth latexmath:[P]-dimensional manifold in
latexmath:[X^\mathcal{N}], since
latexmath:[\| D_\sigma y^\mathcal{N}(\mu) \| \leq C_\sigma \forall \mu \in
    \mathcal{D}, \text{ for any order } |\sigma| \in \mathbb{N}_{+0}]

<1-3>Approximation opportunities: Low-Dimension Manifold

.5

(0,0,0) â (1,0,0); (0,0,0) â (0,1,0); (0,0,0) â (0,0,1)
node[right]latexmath:[Y \equiv H^1(\Omega \subset \mathbb{R}^d)];

.5

Spaces & Bases We define the RB approximation space
latexmath:[X_N =\operatorname*{span}\{\xi^n, 1 \leq n \leq N \},\, 1 \leq N \leq N_{max}]
with linearly independent basis functions
latexmath:[\xi^n \in X,\, 1 \leq n \leq N_{max}] We thus obtain
latexmath:[X_N \subset X, \, {\ensuremath{{\operatorname{dim}}}\xspace}(X_N) = N,\, 1 \leq N \leq N_{max}]
and
latexmath:[X_1 \subset X_2 \subset \ldots X_{N_{max}} (\subset X)]
We denote non-hierarchical RB spaces as
latexmath:[X^{nh}_N, 1 \leq N \leq
Nmax,]
latexmath:[X^{nh}_N \subset X, \, {\ensuremath{{\operatorname{dim}}}\xspace}(X^{nh}_N) = N,\, 1 \leq N \leq N_{max}]

Spaces & Bases - Lagrangian

Parameter Samples:
latexmath:[\mbox{\alert{Sample}}: \ \ S_N  = \{ \mu_1 \in \mathcal{D}^{\mu},
    \ldots, \mu_N \in \mathcal{D}^{\mu} \}\quad 1 \leq N \leq N_{\mathrm{max}},]
with
latexmath:[S_1 \subset S_2 \ldots S_{N_\mathrm{max}-1} \subset S_{N_\mathrm{max}} \subset {\ensuremath{\mathcal{D}^\mu}\xspace}]
Lagrangian Hierarchical Space
latexmath:[W_N  =  {\rm span} \: \{ \xi^n \equiv      \underbrace{ u (\mu^n)}_{u^{\mathcal{N}} (\mu^n)}, n = 1, \ldots, N \}.]
with
latexmath:[W_1 \subset W_2 \ldots \subset W_{N_\mathrm{max}} \subset X^{\mathcal{N}} \subset{X}]

Sampling strategies?

* Equidistributed points in latexmath:[\mathcal{D}^\mu](curse of
dimensionality)
* Log-random distributed points in latexmath:[\mathcal{D}^\mu]
* See later for more efficient, adaptive strategies

Space & Bases - Taylor & Hermite

* Taylor reduced basis spaces:
latexmath:[W^{Taylor}_N = \operatorname*{span}\{D_\sigma u(\mu), \forall \sigma \in I^{P,N-1}    \}, 1 \leq N \leq N_{max},]
field variable sensitivity derivatives
* Hermite reduced basis spaces:
latexmath:[W^{Hermite}_N ``='' W^{Lagrangian}_N \cup W^{Taylor}_N]
field variable sensitivity derivatives

Note: We will exclusively use Lagrangian RB spaces in this course.

Space & Bases - Orthogonal Basis Given
latexmath:[\xi^n = u(\mu^n), 1 \leq n \leq N_{max}] (Lagrange case) we
construct the basis set latexmath:[\{\zeta^n, 1 \leq n \leq Nmax\}],
from

^1 = ^1/^1_X; +
for n = 2 : Nmax +
z^n =^n- _m=1^n-1 (^n,^m )_X ^m; +
^n = z^n/z^n_X; +
end. +

Note:
latexmath:[(\zeta^n,\zeta^m)_X =  \delta_{nm}, 1 \leq n,m \leq Nmax]

Space & Bases - Orthogonal Basis Given reduced basis space
latexmath:[X_N  =  {\rm span} \: \{ \zeta^n,  n = 1, \ldots, N \}, 1 \leq N
    \leq N_{max}] we can express any latexmath:[w_N \in X_N] as
latexmath:[w_N = \sum_{k=1}^N {w_N}_n \zeta^n] for unique
latexmath:[{w_N}_n \in \mathbb{R}, 1 \leq n \leq N]

Reduced basis ``matrices''
latexmath:[Z_N \in \mathbb{R}^{\mathcal{N}\times N} , 1
\leq N \leq N_{max}:]
latexmath:[Z_N=[\zeta^1,\zeta^2,...,\zeta^N],  1 \leq N \leq N_{max}]
where, from orthogonality, latexmath:[Z^T_{N_{max}} X Z^T_{N_{max}} =
I_{N_{max}},] and latexmath:[I_M] is the Identity matrix in
latexmath:[\mathbb{R}^{M\times M}].

Formulation (Linear Compliant Case): a Galerkin method

Galerkin Projection Given latexmath:[\mu \in \mathcal{D}^{\mu} ]
evaluate

latexmath:[\label{eq:1}
         s_N (\mu) =  f(u_N (\mu);\mu)]

where latexmath:[u_N (\mu) \in  X_N] satisfies

latexmath:[a (u_N (\mu), v; \mu)  =  f(v;\mu), \ \forall \: v \in X_N \ .]

Formulation (Linear Compliant Case): Optimality For any
latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}], we have the
following optimality results (thanks to Galerkin)
latexmath:[\begin{aligned}
    |||u(\mu) - u_N(\mu)|||_{\mu} &= \inf_{v_N \in X_N} |||u(\mu) -    v_N(\mu)|||_\mu,\\
    ||u(\mu) - u_N(\mu)||_X &\leq \sqrt{\frac{\gamma(\mu)}{\alpha(\mu)}} \inf_{v_N \in X_N} ||u(\mu) -    v_N(\mu)||_X,\\
  \end{aligned}] and latexmath:[\begin{aligned}
    s(\mu)-s_N(\mu) &= |||u(\mu) -    u_N(\mu)|||^{\alert{2}}_\mu,\\
    &= \inf_{v_N \in X_N} |||u(\mu) -    v_N(\mu)|||^{\alert{2}}_\mu,
  \end{aligned}] and finally
latexmath:[0 \leq s(\mu)-s_N(\mu) \leq \gamma(\mu)\inf_{v_N \in X_N} ||u(\mu) -    v_N(\mu)||^{\alert{2}}_X]

Formulation (Linear Compliant Case): offline-online decomposition our RB
approximations: latexmath:[\begin{aligned}
    u_N(\mu)\ =&\ \sum_{j=1}^N\ {u_N}_j(\mu)\ \zeta_j\label{eq:4}
  \end{aligned}]

latexmath:[s_N(\mu)] latexmath:[\label{eq:5}
    \displaystyle s_N(\mu) = \displaystyle\sum_{j=1}^N {u_N}_j(\mu)\ \left\{ \sum_{q=1}^{Q_f}\ \Theta^q_f(\mu)\ f^q(\zeta_j)\right\}]
where latexmath:[{u_N}_i(\mu), 1 \leq i \leq N] satisfies
latexmath:[\begin{aligned}
    \sum_{j=1}^N \left\{ \sum_{q=1}^{Q_a}\ \Theta^q_a(\mu)\ a^q( \zeta_i, \zeta_{j})  \right\} {u_N}_j(\mu) =& \sum_{q=1}^{Q_f}\ \Theta^q_f(\mu)\ f^q(\zeta_i),\notag  \\
    &  1 \leq i \leq N    \label{eq:6}\\
  \end{aligned}]

Formulation (Linear Compliant Case): matrix form

Solve latexmath:[\label{eq:10}
    \underline{A}_N (\mu) \: \underline{u}_N (\mu) = \underline{F}_N]

where latexmath:[\begin{aligned}
      (A_N)_{i \: j} (\mu) &= \sum_{q=1}^{Q_a}\ \Theta^q_a(\mu)\ a^q( \zeta_i, \zeta_{j}) , \\
      & \\
      F_{N \: i} &=  \sum_{q=1}^{Q_f}\ \Theta^q_f(\mu) f^q (\zeta_i) \ . \\[.5ex]
      & 1 \leq i,j \leq N, \quad 1 \leq i \leq N
  \end{aligned}]

Formulation (Linear Compliant Case): complexity analysis

Offline: independent of latexmath:[\mu]

* Solve: latexmath:[N] FEM system depending on
latexmath:[\mathcal{N}]
* Form and store: latexmath:[f^q (\zeta_i)]
* Form and store: latexmath:[a^q( \zeta_i, \zeta_{j})]

Online: independent of latexmath:[\mathcal{N}]

* Given a new
latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]
* Form and solve latexmath:[A_N(\mu)] : latexmath:[O(Q N^2)] and
latexmath:[O(N^3)]
* Compute latexmath:[s_N(\mu)]

Online: latexmath:[N << \mathcal{N}] Online we realize often orders of
magnitude computational economies relative to FEM in the context of

Formulation (Linear Compliant Case): Condition number

[prop:1] Thanks to the orthonormalization of the basis function, we have
that the condition number of latexmath:[A_N(\mu)] is bounded by the
ratio latexmath:[\gamma(\mu)/\alpha(\mu)].

* Write the Rayleigh Quotient
latexmath:[\frac{v_N^T A_N(\mu) v_N}{v_N^T v_N}, \quad \forall v_N \in \mathbb{R}^N]
* Express latexmath:[v_N = \sum_{n=1}^N v_{N_n} \zeta^n]
* Use coercivity, continuity and orthonormality.

[[sec:post-error-estim]]
A Posteriori Error Estimation
-----------------------------

[[motivations-preliminaries]]
Motivations & Preliminaries
~~~~~~~~~~~~~~~~~~~~~~~~~~~

``Truth'' Problem statement

Let latexmath:[\mu \in \mathcal{D}^{\mu}], evaluate
latexmath:[\displaystyle s (\mu) = \ell (u (\mu)) \ ,] where
latexmath:[u (\mu) \in X] satisfies
latexmath:[a (u (\mu), v; \mu ) = f (v), \quad \forall \: v \in X \ .]
Assumptions

* linearity, coercivity, continuity
* affine parameter dependence; and
* compliance: latexmath:[\ell=f], latexmath:[a] symmetric

Reduced Basis Sample and Space

Parameter Samples:
latexmath:[\mbox{\alert{Sample}}: \ \ S_N  = \{ \mu_1 \in \mathcal{D}^{\mu},
    \ldots, \mu_N \in \mathcal{D}^{\mu} \}\quad 1 \leq N \leq N_{\mathrm{max}},]
with
latexmath:[S_1 \subset S_2 \ldots S_{N_\mathrm{max}-1} \subset S_{N_\mathrm{max}} \subset {\ensuremath{\mathcal{D}^\mu}\xspace}]
Lagrangian Hierarchical Space
latexmath:[W_N  =  {\rm span} \: \{ \xi^n \equiv      \underbrace{ u (\mu^n)}_{u^{\mathcal{N}} (\mu^n)}, n = 1, \ldots, N \}.]
with
latexmath:[W_1 \subset W_2 \ldots \subset W_{N_\mathrm{max}} \subset X^{\mathcal{N}} \subset{X}]

Reduced basis approximation Given
latexmath:[\mu \in \mathcal{D}^{\mu} ] evaluate

latexmath:[\label{eq:1}
    s_N (\mu) =  f(u_N (\mu);\mu)]

where latexmath:[u_N (\mu) \in  X_N] satisfies

latexmath:[a (u_N (\mu), v; \mu)  =  f(v;\mu), \ \forall \: v \in X_N \ .]
Recall:

* RB Space: latexmath:[X_N=``\text{Gram-Schmidt}''(W_N)]
* latexmath:[u_N(\mu)] unique (coercivity, continuity, linear
dependence)

Coercivity and Continuity Constants

We assume latexmath:[a] and

Recall that

* constant
latexmath:[(0 < ) \alpha(\mu) \equiv \inf_{v\in X}\frac{a(v,v;\mu)}{||v||^2_X}]
* constant
latexmath:[\gamma(\mu) \equiv \sup_{w\in X} \sup_{v\in X}\frac{a(w,v;\mu)}{\|w\|_X
      \|v\|_X} ( < \infty)]

Affine dependence and parametric coercivity We assume that
latexmath:[a: X\times X \times \mathcal{D} \rightarrow
  \mathbb{R}] is

* latexmath:[a(u,v;\mu) = \sum_{q=1}^{Q_a} \Theta^q_a(\mu)\ a^q( u, v ),\,
    \forall u,v \in X]
* and
latexmath:[\Theta^q_a(\mu) > 0\quad \forall \mu \in \mathcal{D}, \, 1 \leq q \leq Q_a]
and
latexmath:[a^q(u,v) \geq 0\quad \forall u,v \in X, \, 1 \leq q \leq Q_a]

Inner Products and Norms

* and associated norm () latexmath:[\begin{aligned}
      (((w,v)))_\mu &=  a(w,v;\mu) &\ \forall u,v \in X\\
      |||v|||_\mu &=  \sqrt{a(v,v;\mu)} &\ \forall v \in X
    \end{aligned}]
* latexmath:[X]-inner product and associated norm ()
latexmath:[\begin{aligned}
      (w,v)_X &=  (((w,v)))_{\bar{\mu}} \ (\equiv a(w,v;\bar{\mu})) &\ \forall u,v \in X\\
      ||v||_X &=  |||v|||_{\bar{\mu}} \ (\equiv \sqrt{a(v,v;\bar{\mu})}) & \ \forall v \in X
    \end{aligned}]

[[bound-theorems]]
Bound theorems
~~~~~~~~~~~~~~

Questions

* What is the accuracy of latexmath:[u_N(\mu)] and
latexmath:[s_N(\mu)] ? Online latexmath:[\begin{aligned}
      \|u(\mu)-u_N(\mu)\|_X &\leq \epsilon_{\mathrm{tol}}, \quad \forall \mu \in
      {\ensuremath{\mathcal{D}^\mu}\xspace}\\
      |s(\mu)-s_N(\mu)\|_X &\leq \epsilon^s_{\mathrm{tol}}, \quad \forall \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}\\
    \end{aligned}]
* What is the best value for latexmath:[N] ? Offline/Online
** latexmath:[N] too large latexmath:[\Rightarrow] computational
inefficiency
** latexmath:[N] too small latexmath:[\Rightarrow] unacceptable
error
* How should we build latexmath:[S_N] ? is there an optimal
construction ? Offline
** Good approximation of the manifold latexmath:[\mathcal{M}] through
the RB space, but
** need for well conditioned RB matrices

A Posteriori Error Estimation: Requirements We shall develop the
following error bounds latexmath:[{\ensuremath{\Delta_N(\mu)}\xspace}]
and latexmath:[\Delta^s_N(\mu)] with the following properties

* latexmath:[1 \leq N \leq N_{\mathrm{max}}]
latexmath:[\begin{aligned}
      \|u(\mu)-u_N(\mu)\|_X &\leq \Delta_N(\mu), \quad \forall \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}\\
      |s(\mu)-s_N(\mu)| &\leq \Delta^s_N(\mu), \quad \forall \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}\end{aligned}]
* latexmath:[1 \leq N \leq N_{\mathrm{max}}]
latexmath:[\begin{gathered}
      \frac{\Delta_N(\mu)}{\|u(\mu)-u_N(\mu)\|_X} \leq C,
      \frac{\Delta^s_N(\mu)}{|s(\mu)-s_N(\mu)|} \leq C,\\C\approx 1
    \end{gathered}]
* Online cost depend only on latexmath:[Q] and latexmath:[N]

latexmath:[u_N(\mu)] : Error equation and residual dual norm Given our
RB approximation latexmath:[u_N(\mu)], we have
latexmath:[\label{eq:20}
    e(\mu) \equiv u(\mu)  - u_N(\mu)] that satisfies
latexmath:[\label{eq:21}
    a( e(\mu), v; \mu ) \ = \ r( u_N(\mu), v; \mu ), \forall v \in X]
where latexmath:[r( u_N(\mu), v; \mu ) = f(v) - a( u_N(\mu), v; \mu )]
is the . We have then from coercivity and the definitions above that
latexmath:[\label{eq:22}
    ||e(\mu)||_{X} \ \leq\ \frac{||r( u_N(\mu), v; \mu )||_{X'}}{\alpha(\mu)}\ =\ \frac{\varepsilon_N(\mu)}{\alpha(\mu)}]

A Posteriori error estimation: Dual norm of the residual

[prop:1] Given
latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}], the dual
norm of latexmath:[r(u_N(\mu),\cdot;\mu)] is defined as follows
latexmath:[\begin{aligned}
      ||r(u_N(\mu),\cdot;\mu)||_{X'} & \equiv \sup_{v\in X}
      \frac{r(u_N(\mu),v;\mu)}{||v||_X}\\
      & = ||{\ensuremath{\Hat{e}(\mu)}\xspace}||_X
    \end{aligned}] where
latexmath:[{\ensuremath{\Hat{e}(\mu)}\xspace}\in X] satisfies
latexmath:[\begin{aligned}
      ({\ensuremath{\Hat{e}(\mu)}\xspace},v)_X = r(u_N(\mu),v;\mu)
    \end{aligned}]

The error residual equation can then be rewritten
latexmath:[a( e(\mu), v; \mu ) \ = ({\ensuremath{\Hat{e}(\mu)}\xspace},v)_X, \quad \forall v \in X]

latexmath:[u_N(\mu)] : Definitions of energy error bounds and
effectivity Given
latexmath:[{\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}] a
nonnegative lower bound of
latexmath:[{\ensuremath{\alpha(\mu)}\xspace}]:
latexmath:[\label{eq:23}
    {\ensuremath{\alpha(\mu)}\xspace}\geq {\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}\geq \epsilon_{\alpha} {\ensuremath{\alpha(\mu)}\xspace},\ \epsilon_{\alpha} \ \in\ ]0,1[,\, \forall \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]
Denote
latexmath:[\varepsilon_N(\mu) = \|{\ensuremath{\Hat{e}(\mu)}\xspace}\|_X = \|r(u_N(\mu),v;\mu\|_{X'}]

latexmath:[\label{eq:25}
      \Delta^{\mathrm{en}}_N(\mu) \ \equiv \ \frac{\varepsilon_N(\mu)}{\sqrt{{\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}}}]

latexmath:[\label{eq:25}
      \eta^{\mathrm{en}}_N(\mu) \ \equiv \ \frac{\Delta^{\mathrm{en}}_N(\mu)}{|||e(\mu)|||_\mu}]

latexmath:[u_N(\mu)] : energy error bounds

latexmath:[\label{eq:26}
    1 \ \leq\ \eta^{\mathrm{en}}_N(\mu) \ \leq \sqrt{\frac{{\ensuremath{{\gamma_{{\mathrm{UB}}}}(\mu)}\xspace}}{{\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}}}, \quad 1 \leq N \leq N_{\max}, \quad \forall \mu\ \in \ {\ensuremath{\mathcal{D}^\mu}\xspace}]

Remarks

* : Left inequality ensures rigorous upper bound measured in
latexmath:[||\cdot||_{X}] , i.e.
latexmath:[||e(\mu)||_{X} \leq {\ensuremath{\Delta_N(\mu)}\xspace},\ \forall \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]
* : Right inequality states that
latexmath:[\Delta_N(\mu)]overestimates the ``true'' error by at most
latexmath:[\gamma(\mu) / {\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}]
* for latexmath:[a] and symmetric
latexmath:[\theta^{\bar{\mu}} \equiv
        \frac{\Theta^{\max,\bar{\mu}}_a(\mu)}{\Theta^{\min,\bar{\mu}}_a(\mu)}
        = \frac{\gamma_{\mathrm{ub}}(\mu)}{\alpha_{\mathrm{lb}}(\mu)}]

latexmath:[s_N(\mu)] : output error bounds

latexmath:[1 \ \leq\ \eta^s_N(\mu)  \ \leq \frac{{\ensuremath{{\gamma_{{\mathrm{UB}}}}(\mu)}\xspace}}{{\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}}, \quad 1 \leq N \leq N_{\max}, \quad \forall \mu\ \in \ {\ensuremath{\mathcal{D}^\mu}\xspace}]

where latexmath:[\label{eq:30}
    \Delta^s_N (\mu) = {\Delta_N^{\mathrm{en}}}(\mu)^2] and
latexmath:[\eta^s_N(\mu)\equiv \frac{\Delta^s_N(\mu)}{s(\mu)-s_N(\mu)}]

Rapid convergence of the error in the output Note that the error in the
output vanishes quadratically

Relative output error bounds We define

* the
latexmath:[\Delta^{s,rel}_N (\mu) \equiv \frac{\|\hat{e}(\mu)\|^2_X}{
          \alpha_\mathrm{lb}(\mu) s_N(\mu)}= \frac{\Delta_N^{\mathrm{en}}(\mu)^2}{s_N(\mu)}]
* the
latexmath:[\eta^{s,rel}_N(\mu)\equiv \frac{\Delta^{s,rel}_N(\mu)}{s(\mu)-s_N(\mu)/s(\mu)}]

latexmath:[1 \ \leq\ \eta^{s,rel}_N(\mu)  \ \leq 2 \frac{{\ensuremath{{\gamma_{{\mathrm{UB}}}}(\mu)}\xspace}}{{\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}}, \quad 1 \leq N \leq N_{\max}, \quad \forall \mu\ \in \ {\ensuremath{\mathcal{D}^\mu}\xspace}]

for latexmath:[\Delta^{s,rel}_N \leq 1]

latexmath:[X]-norm error bounds We define

* the
latexmath:[\Delta_N (\mu) \equiv \frac{\|\hat{e}(\mu)\|_X}{\alpha_\mathrm{lb}(\mu)}]
* the
latexmath:[\eta_N(\mu)\equiv \frac{\Delta_N(\mu)}{\|e(\mu)\|_X}]

latexmath:[1 \ \leq\ \eta_N(\mu)  \ \leq \frac{{\ensuremath{{\gamma_{{\mathrm{UB}}}}(\mu)}\xspace}}{{\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}}, \quad 1 \leq N \leq N_{\max}, \quad \forall \mu\ \in \ {\ensuremath{\mathcal{D}^\mu}\xspace}]

Remarks on error bounds Remarks:

* The error bounds are rigorous upper bounds for the reduced basis error
for any latexmath:[N = 1,\ldots,N_{max}] and for all
latexmath:[\mu \in \mathcal{D}].
* The upper bounds for the effectivities are
** independent of latexmath:[N] , and
** independent of latexmath:[\mathcal{N}] if
latexmath:[\alpha_{\mathrm{lb}}(\mu)] only depends on
latexmath:[\mu],
+
and are thus stable with respect to RB and FEM refinement.
* Results for energy norm (and latexmath:[X]-norm) bound directly
extend to noncompliant (& nonsymmetric) problems
** if we choose an appropriate definition for the energy (and
latexmath:[X]) norm

[[offline-online-decomposition]]
Offline-Online decomposition
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Offline-Online decomposition Denote
latexmath:[{\ensuremath{\Hat{e}(\mu)}\xspace}\in X]
latexmath:[\label{eq:34}
    ||{\ensuremath{\Hat{e}(\mu)}\xspace}||_X = \varepsilon_N(\mu) = ||r(u_N(\mu),\cdot;\mu)||_X]
such that latexmath:[\label{eq:36}
    ({\ensuremath{\Hat{e}(\mu)}\xspace},v)_X = -r(u_N(\mu),v;\mu), \quad \forall v \in X]

And recall that latexmath:[\label{eq:35}
    -r(u_N(\mu),v;\mu) = f(v) - \sum_{q=1}^Q \sum_{n=1}^N\ \Theta^q(\mu)\ {u_N}_n(\mu)\ a^q( \zeta_n,v), \quad \forall v\ \in\ X]

Offline-Online decomposition

* It follows next that
latexmath:[{\ensuremath{\Hat{e}(\mu)}\xspace}\in X] satisfies
latexmath:[({\ensuremath{\Hat{e}(\mu)}\xspace},v)_X \ = \ f(v) - \sum_{q=1}^Q \sum_{n=1}^N\ \Theta^q(\mu)\ {u_N}_n(\mu)\ a^q( \zeta_n,v), \quad \forall v\ \in\ X]
* Observe then that the rhs is the _sum_ of products of parameter
dependent functions and parameter independent linear functionals, thus
invoking
latexmath:[{\ensuremath{\Hat{e}(\mu)}\xspace}\ = \ \mathcal{C} - \sum_{q=1}^Q \sum_{n=1}^N\ \Theta^q(\mu)\ {u_N}_n(\mu)\ \mathcal{L}^q_n]
where
** latexmath:[\mathcal{C} \in X] satisfies
latexmath:[(\mathcal{C},v) = f(v), \forall v \in X]
** latexmath:[\mathcal{L} \in X] satisfies
latexmath:[(\mathcal{L}^q_n,v)_X = -a^q(\zeta_n,v), \forall v \in X, \, 1 \leq n \leq N, 1 \leq q \leq Q]
which are parameter independent problems

Offline-Online decomposition: Error bounds From ([eq:12]) we get
latexmath:[\begin{aligned}
    ||{\ensuremath{\Hat{e}(\mu)}\xspace}||_X^2\ =\ & (\mathcal{C},\mathcal{C})_X\ +\ \sum_{q=1}^Q \sum_{n=1}^N\ \Theta^q(\mu)\ {u_N}_n(\mu)\ \displaystyle \Bigg\{ 2 ( \mathcal{C}, \mathcal{L}^q_n)_X \notag\\
      & + \sum_{q'=1}^{Q'} \sum_{n'=1}^{N'}\  \Theta^{q'}(\mu)\ {u_N}_{n'}(\mu)\  ( \mathcal{L}^{q}_{n}, \mathcal{L}^{q'}_{n'})_X \Bigg\}    \label{eq:rbellipticlinear_error:37}
    \end{aligned}]

Remark In ([eq:rbellipticlinear_error:37]),
latexmath:[||{\ensuremath{\Hat{e}(\mu)}\xspace}||_X^2] is the sum of
products of

* and
* ,

the offline-online for the error bounds is now clear.

Offline-Online decomposition: steps and complexity

Offline:

* Solve for latexmath:[\mathcal{C}] and
latexmath:[\mathcal{L}^q_n,\ 1 \leq n \leq N,\ 1 \leq q \leq Q]
* Form and save latexmath:[(\mathcal{C},\mathcal{C})_X],
latexmath:[( \mathcal{C},
      \mathcal{L}^q_n)_X] and
latexmath:[( \mathcal{L}^{q}_{n}, \mathcal{L}^{q'}_{n'})_X],
latexmath:[1 \leq n,n' \leq N,\ 1 \leq q, q' \leq Q]

Online

* Given a new
latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]
* Evaluate the sum
latexmath:[||{\ensuremath{\Hat{e}(\mu)}\xspace}||_X^2]
([eq:rbellipticlinear_error:37]) in terms of latexmath:[\Theta^q(\mu)]
and latexmath:[{u_N}_n(\mu)]
* Complexity in latexmath:[O(Q^2 N^2)] independent of
latexmath:[\mathcal{N}]

[[sec:post-error-estim-1]]
Sampling strategy: a Greedy algorithm
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Offline-Online Scenarii

Offline Given a tolerance latexmath:[\tau], build latexmath:[S_N]
and latexmath:[W_N] s.t.
latexmath:[\forall \ \mu\ \in \mathcal{P} \equiv \mathcal{D}^{\mu} \ , \ \Delta_N(\mu) < \tau]

Online Given latexmath:[\mu] and a tolerance latexmath:[\tau], find
latexmath:[N^*] and thus latexmath:[s_{N^*}(\mu)] s.t.
latexmath:[N^* = \operatorname{arg\ max}_N\ \left( \Delta_{N}(\mu) < \tau \right)]

or given latexmath:[\mu] and a max execution time latexmath:[T],
find latexmath:[N^*] and thus latexmath:[s_{N^*}(\mu)] s.t.
latexmath:[N^* = \operatorname{arg\ min}_N\ \left( \Delta_{N}(\mu) \mbox{ and execution time } < T   \right)]

latexmath:[S_N] and latexmath:[W_N] Generation Strategies

Offline Generation

* Given a tolerance latexmath:[\epsilon], set latexmath:[N = 0] and
latexmath:[S_0 = \emptyset]
* While
latexmath:[{\ensuremath{\Delta_N^{\mathrm{max}}}\xspace}> \epsilon]
* latexmath:[N = N+1]
* If N == 1; then Pick ((log-)randomly)
latexmath:[\mu_1 \in {\ensuremath{\mathcal{D}^\mu}\xspace}]
* Build
latexmath:[{\ensuremath{S_N}\xspace}:= \{ \mu_N \} \cup S_{N-1}]
* Build
latexmath:[{\ensuremath{W_N}\xspace}:= \{ \xi = u(\mu_N) \} \cup W_{N-1}]
* Compute
latexmath:[{\ensuremath{\Delta_N^{\mathrm{max}}}\xspace}:= \mathrm{max}_{\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}}\, \Delta_N(\mu)]
* 
* End While

Condition number recall that the latexmath:[\zeta_n] are , this
ensures that the condition number will stay bounded by
latexmath:[\gamma(\mu)/\alpha(\mu)]

Online Algorithm I

latexmath:[\mu] adaptive online

* Given latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}],
compute
latexmath:[({\ensuremath{s_{N^{*}}}\space}(\mu), {\ensuremath{\Delta_{N^{*}}}\space}(\mu))]
such that latexmath:[{\ensuremath{\Delta_{N^{*}}}\space}(\mu) < \tau.]
* latexmath:[N = 2]
* While latexmath:[{\ensuremath{\Delta_N}\space}(\mu) > \tau]
* Compute
latexmath:[({\ensuremath{s_N}\space}(\mu), {\ensuremath{\Delta_N}\space}(\mu)) \mbox{ using } ({\ensuremath{S_N}\xspace},{\ensuremath{W_N}\xspace})]
* latexmath:[N = N * 2\qquad] +
use the (very) fast convergence properties of RB
* End While

Online Algorithm II

Offline

* While latexmath:[i <= \mathrm{Imax} >> 1]
* Pick log-randomly
latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]
* Store in table
latexmath:[\mathcal{T}, {\ensuremath{\Delta_N}\space}(\mu)] if for
latexmath:[N=1,..., {\ensuremath{{N^{\mathrm{max}}}}\xspace}]
* latexmath:[i = i + 1]; End While

Online Algorithm II â latexmath:[\mu] adaptive online â worst case

* Given latexmath:[\mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}],
compute
latexmath:[({\ensuremath{s_{N^{*}}}\space}(\mu), {\ensuremath{\Delta_{N^{*}}}\space}(\mu))]
such that latexmath:[{\ensuremath{\Delta_{N^{*}}}\space}(\mu) < \tau.]
* latexmath:[N^{*} := \mathrm{arg} \mathrm{max}_{\mathcal{T}}\, {{\ensuremath{\Delta_N}\space}(\mu) \, < \, \tau}]
* Use latexmath:[{\ensuremath{W_{N^{*}}}\xspace}] to compute
latexmath:[({\ensuremath{s_{N^{*}}}\space}(\mu),{\ensuremath{\Delta_{N^{*}}}\space}(\mu))]

[[sec:inf-sup-lower]]
Inf-sup lower bound
-------------------

Lower bound for coercivity constant We require a
latexmath:[{\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}] for
latexmath:[{\ensuremath{\alpha(\mu)}\xspace}= \alpha_c(\mu),\ \forall
  \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]

Two strategies are available:

* ``Min latexmath:[Theta]'' approach if latexmath:[a] is
parametrically coercive (_i.e._ the coercivity constant depends solely
on latexmath:[\mu])
* and more generally the Successive Constraint Method(SCM) which can
also be applied in case of ``Inf-Sup'' stable problems (Stokes,
Helmholtz,...)

[[min-theta-approach]]
âMin latexmath:[\Theta]â Approach
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``Min latexmath:[Theta]'' approach: Lower bound for
latexmath:[\alpha(\mu)]

For a parametrically coercive bilinear form

* latexmath:[\Theta^q(\mu) > 0,\ \forall \mu \in {\ensuremath{\mathcal{D}^\mu}\xspace}]
and
* latexmath:[a^q(v,v) \geq 0,\ \forall v \in X,\ 1 \leq q \leq Q]

We have latexmath:[\label{eq:38}
    \Theta^{\mathrm{min},\Bar{\mu}}_a(\mu) =
    \alpha(\Bar{\mu})\min_{q=1...Q}\frac{\Theta_a^q(\mu)}{\Theta_a^q(\Bar{\mu})} \leq \alpha(\mu)]
where latexmath:[\Bar{\mu} \in {\ensuremath{\mathcal{D}^\mu}\xspace}]
which was used to define the latexmath:[X]-inner product and induced
norm

Recall that latexmath:[\begin{aligned}
    (u,v)_X &=  a(u,v;\alert{\Bar{\mu}}), \quad \forall u,v \in X\\
    \|v\|_X &=  \sqrt{(u,v)_X}, \quad \forall v \in X
  \end{aligned}]

``Min latexmath:[Theta]'' approach: Upper bound for
latexmath:[\gamma(\mu)] Similarly we develop an upper bound
latexmath:[{\ensuremath{{\gamma_{{\mathrm{UB}}}}(\mu)}\xspace}] for
latexmath:[\gamma(\mu)]. We define latexmath:[\label{eq:38}
    \infty > \Theta^{\mathrm{max},\Bar{\mu}}_a(\mu) = \gamma(\Bar{\mu}) \max_{q=1...Q}\frac{\Theta_q^q(\mu)}{\Theta_a^q(\Bar{\mu})}\geq \gamma(\mu)]

[rem:mintheta-rem]
latexmath:[{\ensuremath{{\gamma_{{\mathrm{UB}}}}(\mu)}\xspace}] is
actually not required in practice but relevant in the theory.

``Min latexmath:[Theta]'' approach: Summary if latexmath:[a] is
parametrically coercive we then choose

* the coercivity constant lower bound to be
latexmath:[{\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}\equiv \Theta^{\mathrm{min},\Bar{\mu}}_a(\mu)]
* and the continuity constant upper bound to be (latexmath:[a]
symmetric)
latexmath:[{\ensuremath{{\gamma_{{\mathrm{UB}}}}(\mu)}\xspace}\equiv \Theta^{\mathrm{max},\Bar{\mu}}_a(\mu)]

[rem:mintheta-rem2]

* Online cost to evaluate
latexmath:[{\ensuremath{{\alpha_{{\mathrm{LB}}}}(\mu)}\xspace}] :
latexmath:[O(Q_a)]
* Choice of inner product important latexmath:[(u,v)_X &=
      a(u,v;\alert{\Bar{\mu}})] (see multiple inner products approach)
* Extends to non-symmetric problems by considering the symmetric part
latexmath:[a_s(u,v;\mu) = \frac{1}{2}( a(u,v;\mu)+a(v,u;\mu) )]

[[sec:succ-constr-meth]]
Successive constraint method (SCM)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

[[successive-constraint-method-stability-estimates]]
Successive Constraint method: Stability estimates
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We wish to compute
latexmath:[\alpha_{\mathrm{LB}}: \mathcal{D} \rightarrow \mathbb{R}]
such that latexmath:[\label{eq:38}
    0 < \alpha_{\mathrm{LB}}(\mu) \leq \alpha^{\mathcal{N}}(\mu), \quad \mu \in \mathcal{D}]
and it computation is rapid latexmath:[O(1)] where
latexmath:[\label{eq:39}
    \alpha^{\mathcal{N}}(\mu)= \mathrm{inf}_{w \in X^{\mathcal{N}}} \frac{a(w,w;\mu)}{\|w\|_X^2}]

Computation of latexmath:[\alpha^{\mathcal{N}}(\mu)]
latexmath:[\alpha^{\mathcal{N}}(\mu)] is the minimum eigenvalue of the
following generalized eigenvalue problem latexmath:[\label{eq:40}
      a(w,v;\mu) = \lambda(\mu)\ m(w,v;\mu), \quad (A w = \lambda B w)]
where latexmath:[m(\cdot,\cdot)] is the bi-linear form associated with
latexmath:[\|\cdot\|_X] and latexmath:[B] is the associated matrix.

[[successive-constraint-methodreformulation]]
Successive Constraint method:Reformulation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The problem as a minimization one First recall
latexmath:[\label{eq:41}
    a(w,v;\mu) = \sum_{q=1}^{Q_{a}}\ \theta_q(\mu)\ a_q(w,v)] Hence we
have latexmath:[\label{eq:42}
    \alpha^{\mathcal{N}}(\mu)= \mathrm{inf}_{w \in X^{\mathcal{N}}} \sum_{q=1}{^Q_a}\ \theta_q(\mu) \frac{a_q(w,w)}{\|w\|_X^2}]
and we note latexmath:[\label{eq:43}
    \mathcal{J}^{\mathrm{obj}}(w;\mu) = \sum_{q=1}^{Q_a}\ \theta_q(\mu) \frac{a_q(w,w)}{\|w\|_X^2}]

Reformulation We have the following optimisation problem
latexmath:[\label{eq:44}
    \alpha^{\mathcal{N}}(\mu)= \mathrm{inf}_{y \in \mathcal{Y}} \mathcal{J}^{\mathrm{obj}}(\mu; y)]
where latexmath:[\label{eq:46}
    \mathcal{J}^{\mathrm{obj}}(\mu; y) \equiv \sum_{q=1}^{Q_a}\ \theta_q(\mu) y_q]
and latexmath:[\label{eq:45}
    \mathcal{Y} = \Big\{ y \in \mathbb{R}^{Q_a} |\ \exists w \in X^{\mathcal{N}}\ \mathrm{s.t.}\ y_q = \frac{a_q(w,w)}{\|w\|_{X^{\mathcal{N}}}^2}, 1 \leq q \leq Q_a \Big\}]

We now need to characterize latexmath:[\mathcal{Y}], to do this we
construct two sets latexmath:[\mathcal{Y}_{\mathrm{LB}}] and
latexmath:[\mathcal{Y}_{\mathrm{UB}}] such that
latexmath:[\mathcal{Y}_{\mathrm{UB}}
    \subset \mathcal{Y} \subset \mathcal{Y}_{\mathrm{LB}}] over which
finding latexmath:[\alpha^{\mathcal{N}}(\mu)] is feasible.

Successive Constraint method: Ingredients First we set the design space
for the minimisation problemÂ . We introduce latexmath:[\label{eq:21}
    \mathcal{B} = \prod_{q=1}^{Q_a} \Big[  \mathrm{inf}_{w\in X^{\mathcal{N}}} \frac{a_q(w,w)}{\|w\|_X^2}; \mathrm{sup}_{w\in X^{\mathcal{N}}} \frac{a_q(w,w)}{\|w\|_X^2} \Big]]
latexmath:[\label{eq:22}
    \Xi = \Big\{ \mu_i \in \mathcal{D}; i=1,...,J \Big\}] and
latexmath:[\label{eq:23}
    C_K = \Big\{ \mu_i \in \Xi; i=1,...,K \Big\} \subset \Xi]

latexmath:[\Xi] is constructed using a latexmath:[\frac{1}{2^p}]
division of latexmath:[\mathcal{D}]: in 1D,
latexmath:[0, 1; \frac{1}{2}; \frac{1}{4},
    \frac{3}{4};...]. latexmath:[C_K] will be constructed using a
greedy algorithm.

Finally we shall denote latexmath:[P_M(\mu;E)] the set of
latexmath:[M] points closest to latexmath:[\mu] in the set
latexmath:[E]. We shall need this type of set to construct the lower
bounds.

[[successive-constraint-method-lower-bounds-mathcaly_mathrmlb]]
Successive Constraint method: Lower bounds
latexmath:[\mathcal{Y}_{\mathrm{LB}}]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Given latexmath:[M_\alpha, M_+ \in \mathbb{N}] we are now ready to
define latexmath:[\mathcal{Y}_{\mathrm{LB}}]
latexmath:[\begin{gathered}
    \label{eq:24}
    \mathcal{Y}_{\mathrm{LB}}(\mu; C_K) = \Big\{ y \in \mathbb{R}^{Q_a}\ |\ y \in  \mathcal{B}, \\
    \ \sum_{q=1}^{Q_a} \theta_q(\mu')  y_q \geq \alpha^{\mathcal{N}}(\mu'),\ \forall \mu' \in P_{M_\alpha}(\mu;C_K) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
    \sum_{q=1}^{Q_a} \theta_q(\mu')  y_q \geq \alpha_{\mathrm{LB}}(\mu';C_{K-1}),\ \forall \mu' \in P_{M_+}(\mu;\Xi\backslash C_K) \Big\}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  \end{gathered}] We now set latexmath:[\label{eq:25}
    \alpha_{\mathrm{LB}}(\mu;C_K) = \mathrm{inf}_{y \in \mathcal{Y}_{\mathrm{LB}(\mu;C_K)}}\ \mathcal{J}^{\mathrm{obj}}(\mu;y)]

Computing latexmath:[\alpha_{\mathrm{LB}}(\mu;C_K)] is in fact a
linear program with latexmath:[Q_a] design variables,
latexmath:[y_q], and latexmath:[2 Q_a+M_\alpha+M_+] constraints
online. It requires the construction of latexmath:[C_K] offline.

[sec:upper-bounds:-mathc]

[[successive-constraint-method-upper-bounds-mathcaly_mathrmub]]
Successive Constraint method: Upper bounds
latexmath:[\mathcal{Y}_{\mathrm{UB}}]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Let latexmath:[\label{eq:26}
    \mathcal{Y}_{\mathrm{UB}}( C_K ) = \Big\{ y^*(\mu_k), 1 \leq k \leq K \Big\}]
with latexmath:[\label{eq:27}
    y^*(\mu) = \mathrm{arg}\mathrm{min}_{y \in \mathcal{Y}}\ \mathcal{J}^{\mathrm{obj}}( \mu; y )]
We set latexmath:[\label{eq:28}
    \alpha_{\mathrm{UB}}( \mu; C_K) = \mathrm{inf}_{y \in \mathcal{Y}_{\mathrm{UB}}(C_K)}\ \mathcal{J}^{\mathrm{obj}}(\mu;y)]

latexmath:[\mathcal{Y}_{\mathrm{UB}}] requires latexmath:[K]
eigensolves to compute the eigenmode latexmath:[\eta_k] associated
with latexmath:[w_k, k=1,...,K] and latexmath:[K Q
    \mathcal{N}] inner products to compute the
latexmath:[y^*_q(w_k)=\frac{a_q(\eta_k,\eta_k;\mu)}{\|\eta_k\|_{X^{\mathcal{N}}}^2},
    k=1,...,K] offline . Then computing
latexmath:[\alpha_{\mathrm{UB}}( \mu; C_K)] is a simple enumeration
online.

[sec:construction-c_k]

Successive Constraint method: latexmath:[C_K]

latexmath:[[C_{K_\mathrm{max}}] = \mathrm{Greedy}(\Xi, \epsilon])
Given latexmath:[\Xi] and latexmath:[\epsilon \in [0;1]]

* While
latexmath:[\mathrm{max}_{\mu \in \Xi}\ \frac{\alpha_{\mathrm{UB}}( \mu; C_K) - \alpha_{\mathrm{LB}}( \mu; C_K)}{\alpha_{\mathrm{UB}}( \mu; C_K)} > \epsilon]
** latexmath:[\mu_{K+1} = \mathrm{arg} \mathrm{max}_{\mu \in \Xi}\ \frac{\alpha_{\mathrm{UB}}( \mu; C_K) - \alpha_{\mathrm{LB}}( \mu; C_K)}{\alpha_{\mathrm{UB}}( \mu; C_K)}]
** latexmath:[C_{K+1} = C_K \cup \{ \mu_{K+1} \}]
** latexmath:[K \leftarrow K+1]
* set latexmath:[K_{\mathrm{max}} = K]

[sec:operations-count]

Successive Constraint method:Offline-Online

latexmath:[\mathrm{Offline}]

* latexmath:[2Q_a+M_\alpha+M_+] eigensolves
latexmath:[\alpha^{\mathcal{N}}(\mu), y^*(\mu_k)] +
* latexmath:[n_\Xi K_{\mathrm{max}} LP(Q,M_\alpha,M_+)] to build
latexmath:[C_{K_{\mathrm{max}}}] +
* latexmath:[K_{\mathrm{max}} Q] inner products over
latexmath:[X^{\mathcal{N}} \Rightarrow \mathcal{Y}_{\mathrm{UB}}]

latexmath:[[\alpha_{\mathrm{LB}}(\mu)] = \mathrm{Online}( \mu, C_{K_{\mathrm{max}}}, M_\alpha, M_+ )]
Given latexmath:[\mu \in \mathcal{D}]

* sort over
latexmath:[C_{K_{\mathrm{max}}} \Rightarrow P_{M_\alpha}(\mu;C_{K_{\mathrm{max}}})]
and latexmath:[P_{M_+}(\mu;\Xi\backslash C_{K_{\mathrm{max}}})]
* latexmath:[(M_\alpha+M_++2) Q_a] evaluation of
latexmath:[\theta_q(\mu')]
* latexmath:[M_\alpha] lookups to get
latexmath:[\mu' \rightarrow \alpha^{\mathcal{N}}(\mu')]
* latexmath:[LP(Q_a,M_\alpha,M+)] to get
latexmath:[\alpha_{\mathrm{LB}} (\mu)]

[[numerical-experiments]]
Numerical Experiments
---------------------

[[problem-statement-2]]
Problem Statement
~~~~~~~~~~~~~~~~~

[[example-thermal-block-heat-transfer-1]]
Example Thermal Block: Heat Transfer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.6

(0,0) rectangle (3,3); (0,0) grid (3,3); (0,0) rectangle (3,3);

(1.5,-0.5)node[right]latexmath:[\Gamma_0] (Heat Flux)
to[out=180,in=90] (1.5,0);
(1.5,3.5)node[right]latexmath:[{\Gamma_{\mathrm{top}}}] (Zero
Dirichlet) to[out=180,in=90] (1.5,3); (3.5,1.5)
node[right]latexmath:[{\Gamma_{\mathrm{sides}}}] (Insulated)
to[out=180,in=90] (0,1.5) ; (3.5,1.5) to[out=180,in=-90] (3,1.5);

in 5mm,15mm,25mm

in 5mm,15mm,25mm

at (+0.45,+0.3) latexmath:[\mu_\theindex];

.3

Example Thermal Block: Problem statement Given
latexmath:[\mu \in (\mu_1,...\mu_P) \in {\ensuremath{\mathcal{D}^\mu}\xspace}\equiv
  [\mu^{\text{min}},\mu^{\text{max}}]^P], evaluate (recall that
latexmath:[\ell = f]) +
latexmath:[s(\mu) = f(u(\mu))] where
latexmath:[u(\mu) \in X \equiv \{ v \in H^1(\Omega), v|_{\Gamma_{\text{top}}}  =  0\}]
satisfies
latexmath:[a(u(\mu), v; \mu) = f(v;\mu) \quad \forall v \in X] we
have latexmath:[P = 8] and given latexmath:[1 < \mu_r < \infty] we
set
latexmath:[\mu^{\mathrm{min}} = 1/\sqrt{\mu_r},\quad \mu^{\mathrm{max}} =
    \sqrt{\mu_r}] such that
latexmath:[\mu^{\mathrm{max}}/\mu^{\mathrm{min}}=\mu_r.]

Example Thermal Block Recall we are in the compliant case
latexmath:[\ell = f], we have
latexmath:[f(v) = \int_{\Gamma_{0}} v\quad \forall v \in X] and
latexmath:[a(u,v;\mu) = \sum_{i=1}^{P} \mu_i \int_{\Omega_i} \nabla u \cdot \nabla v + 1 \int_{\Omega_{P+1}} \nabla u \cdot \nabla v
    \quad\forall u,\ v\ \in X] where
latexmath:[\Omega = \cup_{i=1}^{P+1} \Omega_i].

Example Thermal Block The inner product is defined as follows
latexmath:[(u,v)_X = \sum_{i=1}^P \bar{\mu}_i \int_{\Omega_i}\nabla u \cdot \nabla v + 1 \int_{\Omega_{P+1}} \nabla u \cdot \nabla v]
where latexmath:[\bar{\mu}_i] is a . We have readily that
latexmath:[a] is

* 
* latexmath:[0 < \frac{1}{\sqrt{\mu_r}} \leq \mathrm{min}(\mu_1/\bar{\mu}_1, \ldots,
      \mu_P/\bar{\mu}_P,1) \leq \alpha(\mu)]
* and
latexmath:[\gamma(\mu) \leq \mathrm{max}(\mu_1/\bar{\mu}_1, \ldots,
      \mu_P/\bar{\mu}_P,1) \leq \sqrt{\mu_r} < \infty]

and the linear form latexmath:[f] is .

Example Thermal Block: Affine decomposition We
latexmath:[a(u,v;\mu) = \sum_{q=1}^{P+1} \Theta^q(\mu) a^q(u,v)]
with latexmath:[\begin{aligned}
    \Theta^1(\mu) = \mu_1 & & a^1(u,v) = \int_{\Omega_1} \nabla u \cdot \nabla v\\
    & \vdots & \\
    \Theta^P(\mu) = \mu_P & & a^P(u,v) = \int_{\Omega_P} \nabla u \cdot \nabla v\\
    \Theta^{P+1}(\mu) = 1 & & a^{P+1}(u,v) = \int_{\Omega_{P+1}} \nabla u \cdot \nabla v
  \end{aligned}]

Example Thermal Block

* 0.5
** 
+
image:Figures/pngs/veys/thermalblock/33-max.png[image]
+
0.5
** 
+
image:Figures/pngs/veys/thermalblock/33-min.png[image]
* image:Figures/pngs/veys/thermalblock/33-random.png[image]

[[thermal-block-p1]]
Thermal Block latexmath:[P=1]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ExampleThermal Block latexmath:[P=1]

image:thermalblock-P1[image]

We assume latexmath:[1/\mu^{\min}_1=\mu^{\max}_1=\sqrt{\mu_r}=10]
and choose latexmath:[\mathcal{N}=256]

we set latexmath:[\bar{\mu}=1] and have
latexmath:[\Theta^1_a(\mu)=\mu_1,\, \Theta^2_a(\mu) = 1] Thus,
latexmath:[\Theta_a^{\min,\bar{\mu}}(\mu_1)=\min(\mu_1,1)\\
      \Theta_a^{\max,\bar{\mu}}(\mu_1)=\max(\mu_1,1)]

hence

latexmath:[\theta^{\bar{\mu}}(\mu_1) = \max(\frac{1}{\mu_1},\mu_1)]

and
latexmath:[\theta^{\bar{\mu}}(\mu_1) \leq\sqrt{\mu_r} \forall \mu_1 \in \mathcal{D}]

Example: Thermal Block latexmath:[P=1] in [RHP2008]

.Convergence results for latexmath:[P=1]
[cols="^,>,>,>",options="header",]
|=======================================================================
|latexmath:[N] |latexmath:[\Delta^s_{N,\mathrm{max}}(\mu)]
|latexmath:[\eta^s_{N,\mathrm{ave}}]
|latexmath:[\eta^s_{N,\mathrm{max}}]
|1 |7.2084E+00 |2.3417 |3.3305

|2 |4.5371Eâ01 |2.4858 |3.6850

|3 |6.9652Eâ04 |6.2195 |9.8551

|4 |1.3744Eâ07 |3.3219 |7.2632

|5 |3.1140Eâ11 |6.0789 |7.0453
|=======================================================================

Note that: latexmath:[\eta^s_{N,\mathrm{max}}(\mu_1) \leq
  \eta^s_{\mathrm{max,UB}} \equiv \sqrt{\mu_r} = 10]

* Maximum output error bound: latexmath:[\Delta^s_{N,\mathrm{max}}
      = \max_{\mu \in \Xi_{\mathrm{train}}} \Delta^s_N(\mu)]
* Average output effectivity: latexmath:[\eta^s_{N,\mathrm{ave}} =
      \frac{1}{\Xi_\mathrm{train}}\sum_{\mu \in \Xi_{\mathrm{train}}} \eta^s_N(\mu)]
* Maximum output effectivity: latexmath:[\eta^s_{N,\mathrm{max}}
      = \max_{\mu \in \Xi_{\mathrm{train}}} \eta^s_N(\mu)]

[[example-thermal-block-p8]]
Example Thermal Block latexmath:[P=8]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Thermal Block latexmath:[P=8]

* Configuration :
** 47 600 dofs;
** Preconditionner : LU â Solver : MUMPS
** latexmath:[\Xi : ] parameter sampling of dimension 1 000.
* Plot
latexmath:[\ds{ \max_{\mu \in \Xi} \frac{ |s^{\mathcal{N}}(\mu)-s_N(\mu)|}{s^{\mathcal{N}}(\mu)} }]

table[x=NbBasis,y=Max]; table[x=NbBasis,y=Max]; ;

Thermal Block latexmath:[P=8]

* More parameters there are, more rich the problem is;
* Notations :
** latexmath:[e^i(\mu)] is the relative error on the output when
latexmath:[i] parameters vary.

table[x=NbBasis,y=Max]; table[x=NbBasis,y=Max]; table[x=NbBasis,y=Max];
table[x=NbBasis,y=Max]; table[x=NbBasis,y=Max]; table[x=NbBasis,y=Max];
table[x=NbBasis,y=Max]; table[x=NbBasis,y=Max]; ;
