= Data Management

This document details how files are retrieved and produced within {feelpp}.
The section <<_description, Description>> will detail the {feelpp} input
and output data generated.
The different file formats are detailed in section <<_types_of_data,Types Of Data>>.
The different data storage tools are detailed in section <<_data_in_feel, Data
in {feelpp}>>.

== Description

=== Geometry and Mesh

To run a simulation, {feelpp} needs to works with different input dataets.
{feelpp} can use either a geometry file issued from CAD/CAO or directly
a mesh file. A geometry/mesh file represents the domain of the study and is
a requirement.

NOTE: The mesh file can potentially have a consequential size.

=== Fields

A field file represents a function defined on a mesh, which is associated to the FunctionSpace concept.
This files can be used in many context : restart a simulation, define an initial solution from another simulation, or in post-processing,
// link to the class definition in the adequat part of the user book

=== Configuration files

Configuration files are required to setup the simulation : mesh files, physical model, solution strategy, post-processing, ...

NOTE: In particular, it indicates where the data are located.

=== Visualization

During the simulation, {feelpp} produces data outputs that can be visualized
with post-processing softwares and exported in different formats.


NOTE: Depending on the application, the outputs can have a consequential size!

=== Database

{feelpp} can produce several kinds of databases, depending on the context (e.g. Certified Reduced Basis, POD).
Once generated, these databases can be stored on a distant server or saved statically via DB dump.

//But also in the benchmark system (See <<_benchmark,Benchmark>>  section for details).
//These database are stored on a distant server or can be saved statically via DB dump.

=== Logging

A logging system monitors the execution of the simulation. This logging system
helps the user determining the reason why the application might have failed.
The logs are saved.

=== Benchmark

Reproducibility is an important and difficult challenge. When all data are
available (inputs/ouputs), reproducing existing simulation results for
verification or validation purpose means to be able to place the simulation in
the same original setting (same user environment, parametrization,
hardware). Often the only information available are the one described in
papers. For example, readers have access to tables, plots and parameters
precised in the results descriptions which may be incomplete.
From the author point of view, the difficulty is to know what should or should
not be presented in the paper so that user may be able to reproduce the paper
main result.

Feel++ introduces a benchmark system to help users first. It allows for better catching and
storing of crucial information on their applications (problem size, hardware, ...),
but also enable users to compare new simulation results based on existing and
validated ones.

We distinguish currently several parts that will interact with storage:

1. <<_journal, The journal system>>
2. <<_checker_system, The checker system>>
3. <<_testing_system, The testing system (ctest)>>

==== Journal

The principle of the {feelpp} journal is to retrieve the simulation information
in order to gather it all in a unique metadata file.
Two storage approaches can be considered and are detailed hereafter.

===== Static file (JSON)

A journal file is generated at the end of the simulation or at intermediate
moments based on user event (step updates).

NOTE: The journal is different from the main logging system, they
serve two different purposes.

By default, these metadata are stored in a unique JSON files (`journal.json`).
This file is stored as part of the output and can be used to make queries.
The journal file is versioned to ensure comparison metadata compatibility
between two simulations.

===== Database (MongoDB)

{feelpp} can use link:https://www.mongodb.com/fr/[mongodb], a NOSQL database
(weak dependency). Each generated journal file represent an entry of the mongodb
database.

==== Checker system

The checker system allows users to check some metrics in order to know whether the
simulation has finished successfully or not. Hence, different norms can
be verified for a set of parameters. This system needs to access a
specific and known database (see <<database_mongodb, Database (MongoDB)>> section).
This can be used for example to compute convergence curves or simply verify that the result matches a
previous simulation run in the exact same settings.

NOTE: This system works in conjonction with the journal system.

==== Testing system

{feelpp} deploys its applications as containers for different dedicated softwares
(docker, singularity). These applications are tested to ensure they are working.
The testing system is based on CTest.
Some test applications may use checker, thus requiring (local/online) database
access.

=== Documentation

Most {feelpp} documentation is generated using the asiidoc format.
link:https://antora.org/[Antora] is used to generate the documentation
link:docs.feelpp.org[]. The man pages are automatically generated in the same
format and detail each application usage.
The documentation is automatically generated and stored on an specific web server.

== Types of data

The following table summarizes the type of data used and generated in {feelpp}:

.{feelpp} type of file
[options="header"]
|===
| Type | format(s)

| <<_geometry_and_mesh,CAO files>>
| .geo , .step

| <<_geometry_and_mesh,Mesh files>>
| HDF5+JSON, all read by GMSH

| <<_fields,Fields files>>
| HDF5, Boost serialization

| <<_configuration_files,Configuration files>>
| CFG, JSON

| <<_visualization,Visualization files>>
| Ensight, Ensight Gold, VTK, VTK.js, CSV

| <<_journal,Journal files>>
| JSON

| <<_database,{feelpp} database>>
| HDF5+JSON, Boost serialization, mongodb dump

| <<_documentation, Documentation files>>
| .adoc, .png, .jpeg, ...


|===


== Data storage

We use two kind of storage, depending on the type of data.
Mesh files, visualization files and simulation journal files are stored in the Girder plateform, the other ones in the Github repository.

=== Girder plateform

Girder is a free and open source web-based data management platform developed by Kitware as part of the Resonant data and analytics ecosystem.
It is a complete back-end (server side) technology that can be used with other applications via its RESTful API, or it can be used via its own front-end (client side web pages and JavaScript).

==== Data organisation

Data can be stored for a particular user or in the concept of collections.
This concept allows for sharing of the data between one or several users (Groups concept).

Data are organized in a hierarchical structure with Folders and Files.
When Folders or Files are defined, an unique ID is attributed at this entities.
These IDs allow for easy access to the data.

A level of permission can be defined for a resource :

* READ permission (can view and download resources)
* WRITE permission (includes READ permission, can edit the properties of a resource)

Collections and folders can defined with attributes :

* Public (meaning viewable even by anonymous users)
* Private (meaning viewable only by those with READ access).

==== Upload/Download data

* REST API
* Web interface
* Girder CLI
* Python Client
* JavaScript libraries

==== Authentication

=== MongoDB

Several MongoDB databases are used as storage. MongoDB storage
can be seen in two ways:

* static (stored local as a file, or local server)
* server (stored on external mongodb server)


=== Github repository

Access to the github file can be achieved by :

* using raw.githubusercontent.com
* REST API


== Data in Feel++

=== Simulations

==== Input data

The inputs required to run a Feel++ simulation are :

* the CAO or the mesh files
* one or several cfg files
* one or several json files

These files are given to the application by specifying :

* each file separately
* a directory containing each file
* a zip file containing each file

If the data are not available locally, they can be retrieved remotely by specifying :

** Girder : file or folder ID
** Github : relative path in the github repository

==== Output data

The simulation will produce some data:

* Visualization file
* Journal file

:leveloffset: +4

include::benchmark.adoc[]

:leveloffset: -4


=== Pre-Processing

The mesh generation or the mesh partitioning can be expensive steps in time.
We recommand to prepare these kinds of data before to run the simulation.

The mesh is built from a CAO file. The partitioning is generated from the mesh file.

The mesh (eventually the partitioning) is uploaded to the grider plateform.

=== Post-Processing

After the simulation is terminated, data as vtk.js or screenshot of the visualisation can be produced through python scripts.
This data can be uploaded to the Girder plateform.


== Visualization

=== Paraview

ParaView is an open-source, multi-platform data analysis and visualization application.

The data must be available locally.

=== Paraview-Web

ParaViewWeb, the JavaScript library, is a Web framework to build applications with interactive scientific visualization inside the Web browser. Those applications can leverage a VTK and/or ParaView backend for large data processing and rendering, but can also be used on a static Web server like Apache or NGINX. You can even build local command line tools and use your browser to interact with your application.

The data must be available locally on the server side (where the ParaView server is launch).

=== vtk.js

vtk.js is a rendering library made for Scientific Visualization on the Web. It adapts the VTK structure and expertise to bring high performance rendering into your browser.

The data can be available locally or on the web. We can directly give Girder url of ressource for example.

== Documentations
