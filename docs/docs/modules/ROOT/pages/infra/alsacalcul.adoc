= Alsacalcul


include::{partialsdir}/topnote.adoc[]

image::clusters/mesostra.jpg[Mesocenter of Strasbourg,float="right",align="center"]

Alsacalcul is the supercomputer of UniversitÃ© de Strasbourg.

NOTE: For more information see the official web page of link:https://hpc.unistra.fr/[Alsacalcul].

== Prerequisites

* You have to own an account on the machine. See the
link:https://services-numeriques.unistra.fr/les-services-aux-usagers/hpc/acces-aux-ressources.html[cluster documentation]
for the first step on the cluster.
* To learn how to use the cluster, we recommend reading the
link:https://services-numeriques.unistra.fr/les-services-aux-usagers/hpc/acces-aux-ressources.html[cluster documentation]
* To use differents libraries version that match your need, the cluster uses link:http://modules.sourceforge.net/[environment modules]
You should familiarize first to know how to load specific softwares.
* The cluster use link:https://slurm.schedmd.com/quickstart.html[slurm] job supervisor.
You should be familiar with job creation and job submission before going further.

== Feel++ usage

=== Using singularity container

WARNING: singularity is not yet officially ready on the cluster. A module should be available
`module av 2>&1 | grep singularity`

Get a xref:ROOT:user:install/containers.adoc[Feel++ singularity container].

NOTE: To use the container on multi-nodes, be aware performances are networks interface dependant.

A center-specific container is under preparation for easy-to-use access.

=== From source

WARNING: Work in progress

To compile Feel++ on the cluster, you have to load the following required modules:

module purge
module load batch/slurm
module load cmake/cmake-3.4.1
module load libs/boost-1.63.gnu63
module load libs/hdf5-1.8.16.gnu63
module load petsc/petsc-3.7.6.g63                                                                                                                                                                                                            
module load mesh/gmsh-3.0.1.g63
module load compilers/intel17                                                                                                                                                                                                               
module load compilers/clang5
module load mpi/openmpi-2.0.2.gcc63
module unload compilers/gnu63
module load compilers/gnu62                                                                                                                                                                                                          
module load languages/python-2.7.6

NOTE: If Feel++ does not compile using these modules, please post an 
link:https://github.com/feelpp/book.feelpp.org/issues[issue on github].


=== Feel++ slurm job

Sample slurm submission script: (Needs to be filled with user data, when <> is present)

[source,bash]
----
#!/bin/bash
## Options needed to specify where to launch the jobs and the authorization (required)
#SBATCH -p <partition>
#SBATCH -A <AuthGroup>
## Number of cores required
#SBATCH -n <NBC>
## Time until the job is killed (optional)
#SBATCH -t 12:00:00
## Number of cores per node to user (optional)
#SBATCH --tasks-per-node <NBTPN>
## Impose a constraint to the type of node we want (optional)
#SBATCH --constraint <C>
## Required memory size (optional
#SBATCH --mem=<MSIZE>
## Output some data about the processor (optional)
#SBATCH --cpu_bind=verbose
## Send a mail at the end of the execution (optional)
#SBATCH --mail-type=END
#SBATCH --mail-user=ancel@math.unistra.fr

# Source the config
cd /usr/local/feelpp/config/etc
source feelpprc.sh
cd -

# Load a compiler profile
module load gnu481.profile

# Finally launch the job
# mpirun of openmpi is natively interfaced with Slurm
# No need to precise the number of processors to use
cd ${HOME}/git/feelpp/build_gcc/doc/manual/tutorial
mpirun <myapp>

##  The quick disk is on /workdir (17 To free at this time)

## Job submissions

### Interactive
----

To submit jobs interactively for example on 10 cores, type

[source,bash]
----
salloc -n 10
----

now you have allocated 10 cores and you are ready to launch interactively with ```mpirun```

[source,bash]
----
mpirun --bind-to-core -x LD_LIBRARY_PATH  -x IMPORTANT_VAR /where/is/my/app --option1=arg1 --option2=arg2
----

=== Batch

Save it in myBatch.slurm

[source,bash]
----
#! /bin/bash

#SBATCH -n 2048 #need 2048 cores (one thread by core)
source $HOME/.bash_profile  #or source ~/myEnv
export IMPORTANT_VAR=important_value
cd /workdir/math/`whoami`
mpirun --bind-to-core -x LD_LIBRARY_PATH  -x IMPORTANT_VAR /where/is/my/app --option1=arg1 --option2=arg2
----

To execute, just type:


[source,bash]
----
sbatch myBatch.slurm
----

Note that in {feelpp}, slurm script can be automatically generated by enabling ```FEELPP_ENABLE_SLURM```

== Visualisation process

You can either
 - rsync the ```$workdir``` to your computer
 - use remote visualisation using vnc

=== VNC

 - export view using `ssh -Y`
 - use Chicken of the Vnc (to be installed):
   1. log in hpc-visu
   2. launch the server: /opt/TurboVNC/bin/vncserver
      Enter a password (you choose one)
      see:
      ```New 'X' desktop is hpc-visu-01:6
   3. With chicken on the VNC, log into hpc-visu, display 6 (on that example) and use your password to
      connect.
   4. open a term in the display opened, launch:
     ```vglrun xterm -ls -sb```
      To let the process launched by xterm working on a gpu.
    5. launch ```paraview``` & enjoy.

TIP: ADMIN ONLY - Install new versions of software

New software are installed in /usr/local/feelpp (let's call it FEELPP_IDIR). You need to be in the "math" group to have access to this directory.

=== Clang >= 3.5

Clang is built in the `$FEELPP_IDIR/_clang` directory and installed in the
`$FEELPP_IDIR/clang-X.Y` directory, where X.Y is the current version and
FEELPP_IDIR is the installation directory of applications/libraries built for
feelpp (i.e. /usr/local/feelpp).

Follow the [getting started](http://clang.llvm.org/get_started.html) tutorial
from clang to build the directory structure to build clang.  You can use the
TAG for the clang version you want or use the official latest release of
[clang](http://llvm.org/releases/) than the svn version.

You will stop just before the `../llvm/configure` step.  On hpc-login the
default gcc compiler version is 4.4 and clang >= 3.5 requires at least gcc 4.7.
To take this into account, first load a gcc module for a compiler >= 4.7 (e.g.
compilers/gnu47) and then configure llvm with the following:

[source,bash]
----
../llvm-X.Y.0.src/configure --prefix=/usr/local/feelpp/clang-X.Y --with-gcc-toolchain=$PATH_TO_GCC ```
with PATH_TO_GCC the path where gcc libs, executables and includes are stored.
#Then:
make -j $NJOBS
make install
----

And you should be all set for clang. Don't forget to load the module of the gcc version you used before loading the clang module, otherwise it won't be able to find the correct stdlib version.
